{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "is5ZiquMgj5j",
    "Nt6BQK32gMN1"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 13424334,
     "sourceType": "datasetVersion",
     "datasetId": 8520408
    }
   ],
   "dockerImageVersionId": 31153,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# Configuration variables\ndir = '/kaggle/input/your-data-folder'  # Adjust this path for your Kaggle setup\nimport os\n\nnum_companies = 150 # max is 1026\nnum_days = 1245\nnum_features = 5\nwindow_size = 20 # if you change this is not changed everywhere yet unfortunately todo\ncalculate_correlation = False\ntrain_batch = 1\nval_batch = 1\nK = 5\nepochs = 20\nval_min_num = 10\nuse_kfold = False\n\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n    print(\"Running on Kaggle!\")\n    dir = '/kaggle/input/rsr-dataset/Data/'\n    train_batch = 32\n    val_batch = 32\n    epochs = 100\n    num_companies = 1026\n    calculate_correlation = True\n\nelse:\n    dir = '/home/study/IdeaProjects/Graph-Machine-Learning/Temporal_RSR/data' # Samuel's directory\n    print(\"Running locally!\")\n    # turn\n\nSAVE_PREPROCESSED_DATA = False  # Set to True to save preprocessed data for faster loading\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4fHdjEH4tYB",
    "outputId": "488adae8-79e9-44b9-85a0-e6c06b5b3183",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:25.343691Z",
     "iopub.execute_input": "2025-10-29T17:53:25.344255Z",
     "iopub.status.idle": "2025-10-29T17:53:25.353376Z",
     "shell.execute_reply.started": "2025-10-29T17:53:25.344224Z",
     "shell.execute_reply": "2025-10-29T17:53:25.352576Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:19.293840Z",
     "start_time": "2025-10-29T18:27:19.288603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport numpy as np\nimport networkx as nx\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport os",
   "metadata": {
    "id": "9j8W6h4rkMKv",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:25.354718Z",
     "iopub.execute_input": "2025-10-29T17:53:25.354874Z",
     "iopub.status.idle": "2025-10-29T17:53:29.553412Z",
     "shell.execute_reply.started": "2025-10-29T17:53:25.354862Z",
     "shell.execute_reply": "2025-10-29T17:53:29.552648Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:20.653647Z",
     "start_time": "2025-10-29T18:27:19.362610Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nCOPIED FROM THE PAPER\nsource code: https://github.com/fulifeng/Temporal_Relational_Stock_Ranking\n\"\"\"\ndef load_EOD_data(data_path, market_name, tickers, steps=1):\n    eod_data = []\n    masks = []\n    ground_truth = []\n    base_price = []\n\n    # Determine the expected number of rows based on the first ticker's data\n    first_ticker_path = os.path.join(data_path, market_name + '_' + tickers[0] + '_1.csv')\n    try:\n        first_df = pd.read_csv(first_ticker_path, header=None)\n        num_days = first_df.shape[0] - (1 if market_name == 'NASDAQ' else 0) # Remove last row for NASDAQ\n        num_features = first_df.shape[1] - 1 # Exclude the date column\n    except Exception as e:\n        print(f\"Error reading first ticker file {first_ticker_path}: {e}\")\n        return None, None, None, None\n\n    eod_data = np.zeros([len(tickers), num_days, num_features], dtype=np.float32)\n    masks = np.ones([len(tickers), num_days], dtype=np.float32)\n    ground_truth = np.zeros([len(tickers), num_days], dtype=np.float32) # We're not using this one\n    base_price = np.zeros([len(tickers), num_days], dtype=np.float32)\n\n    for index, ticker in enumerate(tickers):\n        if index % 50 == 0:\n          print(f\"Processed [{index}/{tickers.shape[0]}] tickers\")\n        single_EOD_path = os.path.join(data_path, market_name + '_' + ticker + '_1.csv')\n\n        try:\n            single_df = pd.read_csv(single_EOD_path, header=None)\n            if market_name == 'NASDAQ':\n                single_df = single_df[:-1] # remove the last day since lots of missing data\n\n            # Handle missing values (-1234)\n            single_EOD = single_df.values\n            mask_row_indices, mask_col_indices = np.where(np.abs(single_EOD + 1234) < 1e-8)\n            single_EOD[mask_row_indices, mask_col_indices] = 1.1 # Replace missing values\n\n            # Update masks based on missing closing price\n            missing_close_indices = np.where(np.abs(single_EOD[:, -1] + 1234) < 1e-8)[0]\n            masks[index, missing_close_indices] = 0.0\n\n            eod_data[index, :, :] = single_EOD[:, 1:] # Exclude date column\n            base_price[index, :] = single_EOD[:, -1]\n\n        except Exception as e:\n            print(f\"Error reading ticker file {single_EOD_path}: {e}\")\n            # Mark all days for this ticker as invalid if file reading fails\n            masks[index, :] = 0.0\n\n\n    print('eod data shape:', eod_data.shape)\n    return eod_data, masks, ground_truth, base_price",
   "metadata": {
    "id": "lxx4rTcGrQ51",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:29.554303Z",
     "iopub.execute_input": "2025-10-29T17:53:29.554772Z",
     "iopub.status.idle": "2025-10-29T17:53:29.564819Z",
     "shell.execute_reply.started": "2025-10-29T17:53:29.554746Z",
     "shell.execute_reply": "2025-10-29T17:53:29.563943Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:20.707215Z",
     "start_time": "2025-10-29T18:27:20.702097Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nCOPIED FROM THE PAPER\nsource code: https://github.com/fulifeng/Temporal_Relational_Stock_Ranking\n\"\"\"\ndef load_relation_data(relation_file):\n    relation_encoding = np.load(relation_file)\n    print('relation encoding shape:', relation_encoding.shape)\n    rel_shape = [relation_encoding.shape[0], relation_encoding.shape[1]]\n    mask_flags = np.equal(np.zeros(rel_shape, dtype=int),\n                          np.sum(relation_encoding, axis=2))\n    mask = np.where(mask_flags, np.ones(rel_shape) * -1e9, np.zeros(rel_shape))\n    return relation_encoding, mask",
   "metadata": {
    "id": "152QpGtv3cLe",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:29.565722Z",
     "iopub.execute_input": "2025-10-29T17:53:29.565979Z",
     "iopub.status.idle": "2025-10-29T17:53:29.584380Z",
     "shell.execute_reply.started": "2025-10-29T17:53:29.565962Z",
     "shell.execute_reply": "2025-10-29T17:53:29.583828Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:20.754449Z",
     "start_time": "2025-10-29T18:27:20.751989Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "# Loading data",
   "metadata": {
    "id": "FSE-7G93pDs4"
   }
  },
  {
   "cell_type": "code",
   "source": "# market = \"NYSE\"\nmarket = \"NASDAQ\"",
   "metadata": {
    "id": "2UI3iC-ohQfJ",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:29.586622Z",
     "iopub.execute_input": "2025-10-29T17:53:29.586870Z",
     "iopub.status.idle": "2025-10-29T17:53:29.597891Z",
     "shell.execute_reply.started": "2025-10-29T17:53:29.586843Z",
     "shell.execute_reply": "2025-10-29T17:53:29.597176Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:20.799702Z",
     "start_time": "2025-10-29T18:27:20.798155Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "industry_encodings, industry_mask = load_relation_data(dir+f'/relation/sector_industry/{market}_industry_relation.npy')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fowuc2213lyG",
    "outputId": "27ad7c92-3a47-4f23-fbe3-054969e44eb0",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:29.598625Z",
     "iopub.execute_input": "2025-10-29T17:53:29.598853Z",
     "iopub.status.idle": "2025-10-29T17:53:32.366766Z",
     "shell.execute_reply.started": "2025-10-29T17:53:29.598834Z",
     "shell.execute_reply": "2025-10-29T17:53:32.366010Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:20.959810Z",
     "start_time": "2025-10-29T18:27:20.844747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation encoding shape: (1026, 1026, 97)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "wiki_encodings, wiki_mask = load_relation_data(dir+f'/relation/wikidata/{market}_wiki_relation.npy')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDEWN7RA4Gbt",
    "outputId": "b25d974f-c25b-491f-bbe0-4ddfd7c9e99a",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:32.367567Z",
     "iopub.execute_input": "2025-10-29T17:53:32.367823Z",
     "iopub.status.idle": "2025-10-29T17:53:33.695882Z",
     "shell.execute_reply.started": "2025-10-29T17:53:32.367805Z",
     "shell.execute_reply": "2025-10-29T17:53:33.695144Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:21.037343Z",
     "start_time": "2025-10-29T18:27:20.977776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation encoding shape: (1026, 1026, 43)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "# Load company names\ntickers = np.loadtxt(dir+f'/{market}_tickers.csv', dtype=str)\nprint('tickers shape (# of companies):', tickers.shape)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W220yLcuM08d",
    "outputId": "68fe3e97-6354-4695-d821-2ef503fa6354",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:33.696692Z",
     "iopub.execute_input": "2025-10-29T17:53:33.696988Z",
     "iopub.status.idle": "2025-10-29T17:53:33.706205Z",
     "shell.execute_reply.started": "2025-10-29T17:53:33.696960Z",
     "shell.execute_reply": "2025-10-29T17:53:33.705625Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:21.058511Z",
     "start_time": "2025-10-29T18:27:21.055955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tickers shape (# of companies): (1026,)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "eod_data, eod_masks, eod_ground_truth, eod_base_price = load_EOD_data(dir+\"/2013-01-01\", market, tickers)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fwhOJ--P4jKe",
    "outputId": "70e65b50-499e-4abc-af6a-a98b6fdfe52e",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:33.706954Z",
     "iopub.execute_input": "2025-10-29T17:53:33.707134Z",
     "iopub.status.idle": "2025-10-29T17:53:42.849678Z",
     "shell.execute_reply.started": "2025-10-29T17:53:33.707119Z",
     "shell.execute_reply": "2025-10-29T17:53:42.848944Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:21.955232Z",
     "start_time": "2025-10-29T18:27:21.111585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed [0/1026] tickers\n",
      "Processed [50/1026] tickers\n",
      "Processed [100/1026] tickers\n",
      "Processed [150/1026] tickers\n",
      "Processed [200/1026] tickers\n",
      "Processed [250/1026] tickers\n",
      "Processed [300/1026] tickers\n",
      "Processed [350/1026] tickers\n",
      "Processed [400/1026] tickers\n",
      "Processed [450/1026] tickers\n",
      "Processed [500/1026] tickers\n",
      "Processed [550/1026] tickers\n",
      "Processed [600/1026] tickers\n",
      "Processed [650/1026] tickers\n",
      "Processed [700/1026] tickers\n",
      "Processed [750/1026] tickers\n",
      "Processed [800/1026] tickers\n",
      "Processed [850/1026] tickers\n",
      "Processed [900/1026] tickers\n",
      "Processed [950/1026] tickers\n",
      "Processed [1000/1026] tickers\n",
      "eod data shape: (1026, 1245, 5)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "# Use subset of data for the experiments\nn_companies = 150\n\nwiki_encodings = wiki_encodings[:n_companies, :n_companies, :]\nwiki_mask = wiki_mask[:n_companies, :n_companies]\nindustry_encodings = industry_encodings[:n_companies, :n_companies, :]\nindustry_mask = industry_mask[:n_companies, :n_companies]\n\neod_data, eod_masks, eod_ground_truth, eod_base_price = load_EOD_data(dir+\"/2013-01-01\", \"NASDAQ\", tickers[:n_companies])",
   "metadata": {
    "id": "U1madQ_P7Hq-",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:58:31.973344Z",
     "iopub.execute_input": "2025-10-29T17:58:31.973917Z",
     "iopub.status.idle": "2025-10-29T17:58:32.823635Z",
     "shell.execute_reply.started": "2025-10-29T17:58:31.973896Z",
     "shell.execute_reply": "2025-10-29T17:58:32.822766Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:22.134662Z",
     "start_time": "2025-10-29T18:27:22.016927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed [0/150] tickers\n",
      "Processed [50/150] tickers\n",
      "Processed [100/150] tickers\n",
      "eod data shape: (150, 1245, 5)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": "# Graph based Models",
   "metadata": {
    "id": "2aaU7CdApNk_"
   }
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F",
   "metadata": {
    "id": "wgYbWmhHHA3u",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:42.854837Z",
     "iopub.execute_input": "2025-10-29T17:53:42.855015Z",
     "iopub.status.idle": "2025-10-29T17:53:42.866146Z",
     "shell.execute_reply.started": "2025-10-29T17:53:42.855002Z",
     "shell.execute_reply": "2025-10-29T17:53:42.865576Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:22.145249Z",
     "start_time": "2025-10-29T18:27:22.143260Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# Data Preparation Functions\n# ============================================================================\n\ndef build_adjacency_matrix(industry_encodings, industry_mask, wiki_encodings, wiki_mask, device):\n    \"\"\"\n    Build normalized adjacency matrix from relation encodings and masks\n\n    Args:\n        industry_encodings: [num_companies, num_companies, num_relation_types]\n        industry_mask: [num_companies, num_companies] (-1e9 for no relation, 0 for valid)\n        wiki_encodings: [num_companies, num_companies, num_relation_types]\n        wiki_mask: [num_companies, num_companies]\n\n    Returns:\n        adjacency_matrix: [num_companies, num_companies] - normalized adjacency\n    \"\"\"\n    # Combine relation encodings by summing across relation types\n    industry_adj = torch.sum(industry_encodings, dim=-1)  # [companies, companies]\n    wiki_adj = torch.sum(wiki_encodings, dim=-1)\n\n    # Combine both relation types\n    combined_adj = industry_adj + wiki_adj\n\n    # Apply masks: where mask is -1e9 (no relation), set adjacency to 0\n    combined_mask = industry_mask + wiki_mask\n    combined_adj = torch.where(combined_mask < -1e8, torch.zeros_like(combined_adj), combined_adj)\n\n    # Normalize: row-wise normalization (each row sums to 1)\n    row_sums = combined_adj.sum(dim=1, keepdim=True)\n    adjacency_matrix = combined_adj / (row_sums + 1e-8) # [0, 1)\n\n    return adjacency_matrix.to(device)\n\n\ndef prepare_data(eod_data, masks, base_price, device, window_size=20, prediction_horizon=1):\n    \"\"\"\n    Create sliding windows for time series prediction with mask handling\n\n    Args:\n        eod_data: [num_companies, num_days, num_features]\n        masks: [num_companies, num_days] - 1.0 for valid, 0.0 for missing\n        base_price: [num_companies, num_days] - closing price of stock\n        window_size: Number of historical days to use as input\n        prediction_horizon: Number of days ahead to predict (usually 1)\n\n    Returns:\n        X: Input windows [num_samples, num_companies, window_size, num_features]\n        y: Target returns [num_samples, num_companies, prediction_horizon]\n        sample_masks: Valid sample indicators [num_samples, num_companies]\n    \"\"\"\n    num_companies, num_days, num_features = eod_data.shape\n    num_samples = num_days - window_size - prediction_horizon + 1\n\n    X = torch.zeros(num_samples, num_companies, window_size, num_features, device=device)\n    y = torch.zeros(num_samples, num_companies, prediction_horizon, device=device)\n    sample_masks = torch.zeros(num_samples, num_companies, device=device)\n\n    for i in range(num_samples):\n        X[i] = eod_data[:, i:i+window_size, :]\n        y[i, :, :] = base_price[:, i+window_size : i+window_size+prediction_horizon] #\n\n        # A sample is valid if all days in the window AND the target day are valid\n        window_valid = masks[:, i:i+window_size].min(dim=1)[0]  # [num_companies]\n        target_valid = masks[:, i+window_size : i+window_size+prediction_horizon].min(dim=1)[0]\n        sample_masks[i] = window_valid * target_valid\n\n    return X, y, sample_masks",
   "metadata": {
    "id": "dUaaGygcG3LP",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:42.866844Z",
     "iopub.execute_input": "2025-10-29T17:53:42.867072Z",
     "iopub.status.idle": "2025-10-29T17:53:42.876003Z",
     "shell.execute_reply.started": "2025-10-29T17:53:42.867056Z",
     "shell.execute_reply": "2025-10-29T17:53:42.875243Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:22.193626Z",
     "start_time": "2025-10-29T18:27:22.189417Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True",
   "metadata": {
    "id": "H-6FODpFEYEy",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:42.876889Z",
     "iopub.execute_input": "2025-10-29T17:53:42.877144Z",
     "iopub.status.idle": "2025-10-29T17:53:43.011502Z",
     "shell.execute_reply.started": "2025-10-29T17:53:42.877120Z",
     "shell.execute_reply": "2025-10-29T17:53:43.010747Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:22.498103Z",
     "start_time": "2025-10-29T18:27:22.237438Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\n# Load data\n# num_companies = eod_data.shape[0] # Change if using subset\nnum_companies = 150\nnum_days = 1245\nnum_features = 5\n\n# Subsample to only use the first num_companies\neod_data = torch.tensor(eod_data[:num_companies])\nmasks = torch.tensor(eod_masks[:num_companies])\nprice_prediction = torch.tensor(eod_base_price[:num_companies])  # FIXED: subsample this too!\n\n# Relation data - subsample both dimensions since it's company x company\nindustry_encodings = torch.tensor(industry_encodings[:num_companies, :num_companies])\nindustry_mask = torch.tensor(industry_mask[:num_companies, :num_companies])\nwiki_encodings = torch.tensor(wiki_encodings[:num_companies, :num_companies])\nwiki_mask = torch.tensor(wiki_mask[:num_companies, :num_companies])\n\nprint(f\"EOD data shape: {eod_data.shape}\")\nprint(f\"Masks shape: {masks.shape}\")\nprint(f\"Ground truth shape: {price_prediction.shape}\")\nprint(f\"Industry encodings shape: {industry_encodings.shape}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feQ_WQ2JrkPY",
    "outputId": "1708b013-d82a-4ee3-f1c0-e501f50fef1a",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:54:48.397770Z",
     "iopub.execute_input": "2025-10-29T17:54:48.398489Z",
     "iopub.status.idle": "2025-10-29T17:54:48.999323Z",
     "shell.execute_reply.started": "2025-10-29T17:54:48.398465Z",
     "shell.execute_reply": "2025-10-29T17:54:48.998433Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:22.642295Z",
     "start_time": "2025-10-29T18:27:22.503498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "EOD data shape: torch.Size([150, 1245, 5])\n",
      "Masks shape: torch.Size([150, 1245])\n",
      "Ground truth shape: torch.Size([150, 1245])\n",
      "Industry encodings shape: torch.Size([150, 150, 97])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "def get_adjacency_matrix(prediction_horizon=1, window_size=100):\n  # Build adjacency matrix from relations\n  adjacency_matrix = build_adjacency_matrix(\n      industry_encodings, industry_mask,\n      wiki_encodings, wiki_mask,\n      device=device\n  )\n  #print(f\"Adjacency matrix shape: {adjacency_matrix.shape}\")\n\n  # Prepare temporal data with masks\n  X_train, y_train, train_masks = prepare_data(\n      eod_data, masks, price_prediction,\n      window_size=window_size,\n      device=device,\n      prediction_horizon=prediction_horizon\n  )\n  #print(f\"Training data: X={X_train.shape}, y={y_train.shape}, masks={train_masks.shape}\")\n\n  return (adjacency_matrix, X_train, y_train, train_masks)",
   "metadata": {
    "id": "L8OFp-5bOq6l",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:43.623787Z",
     "iopub.execute_input": "2025-10-29T17:53:43.624106Z",
     "iopub.status.idle": "2025-10-29T17:53:43.629262Z",
     "shell.execute_reply.started": "2025-10-29T17:53:43.624072Z",
     "shell.execute_reply": "2025-10-29T17:53:43.628534Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:22.662180Z",
     "start_time": "2025-10-29T18:27:22.660064Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": "## Simple G-Var",
   "metadata": {
    "id": "3MKh8eCSgqMu"
   }
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nSimple G-VAR (Graph Vector AutoRegression) for Stock Price Prediction\nCombines temporal dependencies (VAR) with graph structure (GNN)\nUpdated to match the paper's data format\n\"\"\"\n\n# ============================================================================\n# Simple G-Var\n# ============================================================================\n\nclass GVarModel(nn.Module):\n    def __init__(self, input_dim, output_dim, num_companies, device, K=2):\n        \"\"\"\n        Args:\n            input_dim: Number of features * window_size per company\n            output_dim: Prediction dimension (1 for return prediction)\n            num_companies: Number of stocks (e.g., 150)\n            K: Number of graph hops\n        \"\"\"\n        super(GVarModel, self).__init__()\n        self.device = device\n        self.K = K\n        self.num_companies = num_companies\n        self.output_dim = output_dim\n\n        self.graph_layers = nn.ModuleList([\n            nn.Linear(input_dim, 1) for _ in range(K + 1)\n        ])\n\n    def forward(self, x, adjacency_matrix):\n        \"\"\"\n        Args:\n            x: Historical data [batch, num_companies, time_steps, input_dim]\n            adjacency_matrix: Graph structure [num_companies, num_companies]\n        Returns:\n            predictions: [batch, num_companies, output_dim]\n        \"\"\"\n        batch_size = x.shape[0]\n\n        # Step 1: Extract temporal features for each stock independently\n        # Reshape to process all companies' time series\n        x_reshaped = x.view(x.shape[0], x.shape[1], -1)  # [batch, companies, time_steps * features]\n\n        # Compute powers of adjacency matrix: A^0 (self), A^1 (neighbors), A^2 (2-hop), ...\n        S_powers = [torch.eye(self.num_companies, device=adjacency_matrix.device)]\n        for k in range(self.K):\n            S_powers.append(torch.matmul(S_powers[-1], adjacency_matrix))\n\n        # Step 2: Aggregate information from k-hop neighbors\n        output = torch.zeros(x.shape[0], x.shape[1], self.output_dim, device=self.device)\n        for k in range(self.K + 1):\n            # Transform features at each hop level\n            transformed = self.graph_layers[k](x_reshaped)  # [batch, companies, hidden_dim]\n\n            # Aggregate from k-hop neighbors: S^k @ transformed\n            aggregated = torch.matmul(S_powers[k], transformed)  # [batch, companies, hidden_dim]\n            output += aggregated\n\n        return output",
   "metadata": {
    "id": "I0WbMW-lHLXW",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:43.630009Z",
     "iopub.execute_input": "2025-10-29T17:53:43.630228Z",
     "iopub.status.idle": "2025-10-29T17:53:43.647858Z",
     "shell.execute_reply.started": "2025-10-29T17:53:43.630207Z",
     "shell.execute_reply": "2025-10-29T17:53:43.647216Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:22.710209Z",
     "start_time": "2025-10-29T18:27:22.706057Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()\n\ntorch.cuda.memory_allocated()\n\n#import gc\n#gc.collect()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Xzxly4ZFtin",
    "outputId": "cc2f23fb-21b6-43ad-c5c8-ae4d4a0d0e99",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:43.648698Z",
     "iopub.execute_input": "2025-10-29T17:53:43.649023Z",
     "iopub.status.idle": "2025-10-29T17:53:43.664586Z",
     "shell.execute_reply.started": "2025-10-29T17:53:43.649008Z",
     "shell.execute_reply": "2025-10-29T17:53:43.663741Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:22.758431Z",
     "start_time": "2025-10-29T18:27:22.755195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "# prediction_horizon=1\n# window_size=100\n\n# adjacency_matrix, X_train, y_train, train_masks = get_adjacency_matrix(prediction_horizon, window_size)\n\n# print(\"train_masks.sum():\", train_masks.sum())\n# print(\"train_masks size:\", train_masks.shape[0]*train_masks.shape[1])\n\n\n# # Initialize model\n# model = GVarModel(\n#     input_dim=num_features*window_size,\n#     output_dim=prediction_horizon, # = prediction_horizon\n#     num_companies=num_companies,\n#     device=device,\n#     K=1\n# ).to(device)\n\n# # Training with masked loss\n# criterion = nn.MSELoss(reduction='none')  # Don't reduce yet, we'll apply masks\n# #criterion = nn.L1Loss(reduction='none')  # Don't reduce yet, we'll apply masks\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# epochs = 500\n# for epoch in range(epochs):\n#     model.train()\n#     optimizer.zero_grad()\n\n#     # Forward pass\n#     #predictions = model(X_train, adjacency_matrix)  # [batch, companies, 1]\n#     predictions = model(X_train, torch.eye(num_companies, device=device))  # [batch, companies, 1]\n\n\n#     # Calculate masked loss (only on valid samples)\n#     loss_per_sample = criterion(predictions, y_train)  # [batch, companies, 1]\n#     masked_loss = loss_per_sample * train_masks.unsqueeze(-1)  # Apply mask\n\n#     # Average loss over valid samples only\n#     num_valid = train_masks.sum() + 1e-8\n#     #loss = masked_loss[:,:,model.output_dim-1].sum() / num_valid # Loss only for prediction_horizon day in future (1 day)\n#     loss = masked_loss.sum() / num_valid # Loss for all days up to prediction_horizon\n\n#     # Backward pass\n#     loss.backward()\n#     optimizer.step()\n\n#     if (epoch + 1) % 50 == 0:\n#         print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')\n\n# print(\"Training compled\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OuT9rVIA8a2q",
    "outputId": "f5e82139-35c1-413c-8120-18e47f62753b",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:43.665394Z",
     "iopub.execute_input": "2025-10-29T17:53:43.665672Z",
     "iopub.status.idle": "2025-10-29T17:53:43.677059Z",
     "shell.execute_reply.started": "2025-10-29T17:53:43.665648Z",
     "shell.execute_reply": "2025-10-29T17:53:43.676246Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:22.805636Z",
     "start_time": "2025-10-29T18:27:22.803765Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": "# GNN",
   "metadata": {
    "id": "KZVrwD7S5NdX"
   }
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\ndef build_graph_matrix(industry_encodings, industry_mask, wiki_encodings, wiki_mask, device):\n    \"\"\"\n    Build adjacency and degree matrices from relation encodings and masks\n\n    Returns:\n        adjacency_matrix: [num_companies, num_companies]\n        degree_matrix: [num_companies, num_companies]\n    \"\"\"\n    # Combine relation encodings by summing across relation types\n    industry_adj = torch.sum(industry_encodings, dim=-1)  # [companies, companies]\n    wiki_adj = torch.sum(wiki_encodings, dim=-1)\n\n    combined_adj = industry_adj + wiki_adj\n\n    combined_mask = industry_mask + wiki_mask\n    #combined_adj = torch.where(combined_mask < -1e8, torch.zeros_like(combined_adj), combined_adj)\n    degree_matrix = torch.diag(torch.pow(torch.sum(combined_adj, dim=1), -0.5))  # [companies, companies]\n\n    graph_shift_operator =  degree_matrix @ combined_adj.float() @  degree_matrix\n    return graph_shift_operator.to(device)\n\ndef prepare_data(eod_data, masks, base_price, device, window_size=20, prediction_horizon=1):\n    \"\"\"\n    Create sliding windows for time series prediction with mask handling\n\n    Returns:\n        X: Input windows [num_samples, num_companies, window_size, num_features]\n        y: Target returns [num_samples, num_companies, prediction_horizon]\n        sample_masks: Valid sample indicators [num_samples, num_companies]\n    \"\"\"\n    num_companies, num_days, num_features = eod_data.shape\n    num_samples = num_days - window_size - prediction_horizon + 1\n\n    X = torch.zeros(num_samples, num_companies, window_size, num_features, device=device)\n    y = torch.zeros(num_samples, num_companies, prediction_horizon, device=device)\n    sample_masks = torch.zeros(num_samples, num_companies, device=device)\n\n    for i in range(num_samples):\n        X[i] = eod_data[:, i:i+window_size, :]\n        y[i, :, :] = base_price[:, i+window_size : i+window_size+prediction_horizon] #\n\n        # A sample is valid if all days in the window AND the target day are valid\n        window_valid = masks[:, i:i+window_size].min(dim=1)[0]  # [num_companies]\n        target_valid = masks[:, i+window_size : i+window_size+prediction_horizon].min(dim=1)[0]\n        sample_masks[i] = window_valid * target_valid\n\n    return X, y, sample_masks\n\nclass GCNModel(nn.Module):\n    def __init__(self, layers_dim, num_companies, S, device, K=1, L=1):\n        \"\"\"\n        Args:\n            input_dim: Number of features * window_size per company\n            output_dim: Prediction dimension (1 for return prediction)\n            num_companies: Number of stocks (e.g., 150)\n            K: Number of graph hops\n        \"\"\"\n        super(GCNModel, self).__init__()\n        self.device = device\n        self.K = K\n        self.L = L\n        self.num_companies = num_companies\n        self.layers_dim = layers_dim\n\n        # Compute powers of adjacency matrix: A^0 (self), A^1 (neighbors), A^2 (2-hop), ...\n        self.S_powers = [S]\n        for k in range(self.K):\n            self.S_powers.append(self.S_powers[-1] @ S)\n\n        self.gcn_layer1 = nn.ModuleList([\n            nn.Linear(layers_dim[0][0], layers_dim[0][1]) for _ in range(K)\n        ])\n\n        self.activation1 = F.ReLU()\n\n        self.gcn_layer2 = nn.ModuleList([\n            nn.Linear(layers_dim[1][0], layers_dim[1][1]) for _ in range(K)\n        ])\n\n        self.activation2 = F.ReLU()\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Historical data [batch, num_companies, time_steps, input_dim]\n            adjacency_matrix: Graph structure [num_companies, num_companies]\n        Returns:\n            predictions: [batch, num_companies, output_dim]\n        \"\"\"\n        batch_size = x.shape[0]\n        x_reshaped = x.view(x.shape[0], x.shape[1], -1)  # [batch, companies, time_steps * features]\n\n        x_i = x_reshaped\n        output = torch.zeros(batch_size, self.num_companies, self.layers_dim[0][1], device=self.device)\n        for k in range(self.K):\n            # Transform features at each hop level\n            transformed = self.gcn_layer1[k](self.S_powers[k] @ x_i)\n\n            # Aggregate from k-hop neighbors using graph structure\n            output += transformed\n        x_i = self.activation1(output)\n\n        output = torch.zeros(batch_size, self.num_companies, self.layers_dim[1][1], device=self.device)\n        for k in range(self.K):\n            # Transform features at each hop level\n            transformed = self.gcn_layer2[k](self.S_powers[k] @ x_i)\n\n            # Aggregate from k-hop neighbors using graph structure\n            output += transformed\n        x_i = self.activation2(output)\n        return x_i",
   "metadata": {
    "id": "85oRacMq5Ms-",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:43.677828Z",
     "iopub.execute_input": "2025-10-29T17:53:43.678036Z",
     "iopub.status.idle": "2025-10-29T17:53:43.695288Z",
     "shell.execute_reply.started": "2025-10-29T17:53:43.678022Z",
     "shell.execute_reply": "2025-10-29T17:53:43.694576Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:22.855795Z",
     "start_time": "2025-10-29T18:27:22.849733Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": "## GCNGATModel",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def build_graph_matrix(industry_encodings, industry_mask, wiki_encodings, wiki_mask, device):\n",
    "    \"\"\"\n",
    "    Build adjacency and degree matrices from relation encodings and masks\n",
    "\n",
    "    Returns:\n",
    "        adjacency_matrix: [num_companies, num_companies]\n",
    "        degree_matrix: [num_companies, num_companies]\n",
    "    \"\"\"\n",
    "    # Combine relation encodings by summing across relation types\n",
    "    industry_adj = torch.sum(industry_encodings, dim=-1)  # [companies, companies]\n",
    "    wiki_adj = torch.sum(wiki_encodings, dim=-1)\n",
    "\n",
    "    combined_adj = industry_adj + wiki_adj\n",
    "\n",
    "    combined_mask = industry_mask + wiki_mask\n",
    "    #combined_adj = torch.where(combined_mask < -1e8, torch.zeros_like(combined_adj), combined_adj)\n",
    "    degree_matrix = torch.diag(torch.pow(torch.sum(combined_adj, dim=1), -0.5))  # [companies, companies]\n",
    "\n",
    "    graph_shift_operator =  degree_matrix @ combined_adj.float() @  degree_matrix\n",
    "    return graph_shift_operator.to(device)\n",
    "\n",
    "def prepare_data(eod_data, masks, base_price, device, window_size=20, prediction_horizon=1):\n",
    "    \"\"\"\n",
    "    Create sliding windows for time series prediction with mask handling\n",
    "\n",
    "    Returns:\n",
    "        X: Input windows [num_samples, num_companies, window_size, num_features]\n",
    "        y: Target returns [num_samples, num_companies, prediction_horizon]\n",
    "        sample_masks: Valid sample indicators [num_samples, num_companies]\n",
    "    \"\"\"\n",
    "    num_companies, num_days, num_features = eod_data.shape\n",
    "    num_companies = 150\n",
    "    num_samples = num_days - window_size - prediction_horizon + 1\n",
    "\n",
    "    X = torch.zeros(num_samples, num_companies, window_size, num_features, device=device)\n",
    "    y = torch.zeros(num_samples, num_companies, prediction_horizon, device=device)\n",
    "    sample_masks = torch.zeros(num_samples, num_companies, device=device)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        X[i] = eod_data[:, i:i+window_size, :]\n",
    "        y[i, :, :] = base_price[:, i+window_size : i+window_size+prediction_horizon] #\n",
    "\n",
    "        # A sample is valid if all days in the window AND the target day are valid\n",
    "        window_valid = masks[:, i:i+window_size].min(dim=1)[0]  # [num_companies]\n",
    "        target_valid = masks[:, i+window_size : i+window_size+prediction_horizon].min(dim=1)[0]\n",
    "        sample_masks[i] = window_valid * target_valid\n",
    "\n",
    "    return X, y, sample_masks\n",
    "\n",
    "class GCNGATModel(nn.Module):\n",
    "    def __init__(self, layers_dim, num_companies, adjacency_matrix, S, device, K=1, L=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Number of features * window_size per company\n",
    "            output_dim: Prediction dimension (1 for return prediction)\n",
    "            num_companies: Number of stocks (e.g., 150)\n",
    "            K: Number of graph hops\n",
    "        \"\"\"\n",
    "        super(GCNGATModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.K = K\n",
    "        self.L = L\n",
    "        self.num_companies = num_companies\n",
    "        self.layers_dim = layers_dim\n",
    "        self.w_linear = nn.Linear(layers_dim[0][0], 15)\n",
    "        self.a = nn.Parameter(torch.randn(size=(2*15, 1))) # size should be 2*out feature size? but what is the out feature size? # todod maybe zero initialize does not work, maybe ones is better\n",
    "        # todo maybe use seed to prevent randomness in initialization\n",
    "        \n",
    "        # todo do i need to initialize the a parameter?\n",
    "\n",
    "\n",
    "        \n",
    "        self.S = S\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "        self.mask = (self.adjacency_matrix == 0)\n",
    "\n",
    "        # Compute powers of adjacency matrix: A^0 (self), A^1 (neighbors), A^2 (2-hop), ...\n",
    "        # self.S_powers = [S]\n",
    "        # for k in range(self.K):\n",
    "        #     self.S_powers.append(self.S_powers[-1] @ S)\n",
    "\n",
    "        # self.gcn_layer1 = nn.ModuleList([\n",
    "        #     nn.Linear(layers_dim[0][0], layers_dim[0][1]) for _ in range(K) # todo I think normal gcn would only have k of 1, and then multiple layers would allow higher k\n",
    "        # ])\n",
    "\n",
    "        # self.activation1 = F.ReLU()\n",
    "\n",
    "        # self.gcn_layer2 = nn.ModuleList([\n",
    "        #     nn.Linear(layers_dim[1][0], layers_dim[1][1]) for _ in range(K)\n",
    "        # ])\n",
    "\n",
    "        # self.activation2 = F.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Historical data [batch, num_companies, time_steps, input_dim]\n",
    "            adjacency_matrix: Graph structure [num_companies, num_companies]\n",
    "        Returns:\n",
    "            predictions: [batch, num_companies, output_dim]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        x_reshaped = x.view(x.shape[0], x.shape[1], -1)  # [batch, companies, time_steps * features]\n",
    "        \n",
    "        x_i = x_reshaped\n",
    "\n",
    "        h = self.w_linear(x_i) # this is first linear transformation of the x input \n",
    "        # so now we have all hi*W, but now we need to concatenate all pairs for in the \n",
    "\n",
    "        # h contains all node(companies) embeddings\n",
    "        # first calculate the first part dot procut with a, after the second part\n",
    "        first_part = h @ self.a[:15, :]\n",
    "        second_part = h @ self.a[15:, :] # check if i should mention inspiration https://epichka.com/blog/2023/gat-paper-explained/\n",
    "        # print(first_part.shape)\n",
    "        # print(second_part.shape)\n",
    "        partly = first_part+ second_part.mT\n",
    "        # print(partly.shape)\n",
    "        e = F.leaky_relu(partly)\n",
    "        # this above was the a^t[Whi || Whj]\n",
    "        \n",
    "        # print(e.shape)\n",
    "\n",
    "        # print(mask.shape)\n",
    "        e = e.masked_fill(self.mask, float('-inf'))\n",
    "        attention = F.softmax(e, dim=-1)\n",
    "        output = attention @ h\n",
    "        \n",
    "        \n",
    "        # output = torch.zeros(batch_size, self.num_companies, self.layers_dim[0][1], device=self.device)\n",
    "        # for k in range(self.K):\n",
    "        #     # Transform features at each hop level\n",
    "        #     transformed = self.gcn_layer1[k](self.S_powers[k] @ x_i)\n",
    "\n",
    "        #     # Aggregate from k-hop neighbors using graph structure\n",
    "        #     output += transformed\n",
    "        # x_i = self.activation1(output)\n",
    "\n",
    "        # output = torch.zeros(batch_size, self.num_companies, self.layers_dim[1][1], device=self.device)\n",
    "        # for k in range(self.K):\n",
    "        #     # Transform features at each hop level\n",
    "        #     transformed = self.gcn_layer2[k](self.S_powers[k] @ x_i)\n",
    "\n",
    "        #     # Aggregate from k-hop neighbors using graph structure\n",
    "        #     output += transformed\n",
    "        # x_i = self.activation2(output)\n",
    "        return output"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:58:40.470365Z",
     "iopub.execute_input": "2025-10-29T17:58:40.470885Z",
     "iopub.status.idle": "2025-10-29T17:58:40.482898Z",
     "shell.execute_reply.started": "2025-10-29T17:58:40.470862Z",
     "shell.execute_reply": "2025-10-29T17:58:40.482205Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:34:49.890077Z",
     "start_time": "2025-10-29T18:34:49.883282Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "window_size=30\n",
    "graph_shift_operator = build_graph_matrix(industry_encodings, industry_mask, wiki_encodings, wiki_mask, device)\n",
    "adjacency_matrix = build_adjacency_matrix(\n",
    "      industry_encodings, industry_mask,\n",
    "      wiki_encodings, wiki_mask,\n",
    "      device=device\n",
    "  )\n",
    "X_train, y_train, train_masks = prepare_data(\n",
    "    eod_data=eod_data,\n",
    "    masks=masks,\n",
    "    base_price=price_prediction,\n",
    "    device=device,\n",
    "    window_size=window_size)\n",
    "\n",
    "# Initialize model\n",
    "model = GCNGATModel(\n",
    "    layers_dim=[(num_features*window_size, 15), (15, 1)],\n",
    "    num_companies=num_companies,\n",
    "    adjacency_matrix=adjacency_matrix,\n",
    "    S=graph_shift_operator,\n",
    "    device=device,\n",
    "    K=1,\n",
    "    L=1\n",
    ").to(device)\n",
    "\n",
    "# Training with masked loss\n",
    "criterion = nn.MSELoss(reduction='none')  # Don't reduce yet, we'll apply masks\n",
    "#criterion = nn.L1Loss(reduction='none')  # Don't reduce yet, we'll apply masks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    #predictions = model(X_train, adjacency_matrix)  # [batch, companies, 1]\n",
    "    predictions = model(X_train)  # [batch, companies, 1]\n",
    "\n",
    "    # Calculate masked loss (only on valid samples)\n",
    "    loss_per_sample = criterion(predictions, y_train)  # [batch, companies, 1]\n",
    "    masked_loss = loss_per_sample * train_masks.unsqueeze(-1)  # Apply mask\n",
    "\n",
    "    # Average loss over valid samples only\n",
    "    num_valid = train_masks.sum() + 1e-8\n",
    "    #loss = masked_loss[:,:,model.output_dim-1].sum() / num_valid # Loss only for prediction_horizon day in future (1 day)\n",
    "    loss = masked_loss.sum() / num_valid # Loss for all days up to prediction_horizon\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "\n",
    "print(\"Training complted\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:58:43.304831Z",
     "iopub.execute_input": "2025-10-29T17:58:43.305467Z",
     "iopub.status.idle": "2025-10-29T17:58:43.329410Z",
     "shell.execute_reply.started": "2025-10-29T17:58:43.305444Z",
     "shell.execute_reply": "2025-10-29T17:58:43.328527Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:37:24.283602Z",
     "start_time": "2025-10-29T18:34:51.941892Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1215, 150, 1])) that is different to the input size (torch.Size([1215, 150, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.620821\n",
      "Epoch [20/1000], Loss: 0.599193\n",
      "Epoch [30/1000], Loss: 0.251606\n",
      "Epoch [40/1000], Loss: 0.091520\n",
      "Epoch [50/1000], Loss: 0.091906\n",
      "Epoch [60/1000], Loss: 0.060357\n",
      "Epoch [70/1000], Loss: 0.059873\n",
      "Epoch [80/1000], Loss: 0.056290\n",
      "Epoch [90/1000], Loss: 0.055382\n",
      "Epoch [100/1000], Loss: 0.054681\n",
      "Epoch [110/1000], Loss: 0.054128\n",
      "Epoch [120/1000], Loss: 0.053693\n",
      "Epoch [130/1000], Loss: 0.053249\n",
      "Epoch [140/1000], Loss: 0.052836\n",
      "Epoch [150/1000], Loss: 0.052438\n",
      "Epoch [160/1000], Loss: 0.052053\n",
      "Epoch [170/1000], Loss: 0.051680\n",
      "Epoch [180/1000], Loss: 0.051318\n",
      "Epoch [190/1000], Loss: 0.050967\n",
      "Epoch [200/1000], Loss: 0.050626\n",
      "Epoch [210/1000], Loss: 0.050295\n",
      "Epoch [220/1000], Loss: 0.049973\n",
      "Epoch [230/1000], Loss: 0.049660\n",
      "Epoch [240/1000], Loss: 0.049355\n",
      "Epoch [250/1000], Loss: 0.049058\n",
      "Epoch [260/1000], Loss: 0.048768\n",
      "Epoch [270/1000], Loss: 0.048485\n",
      "Epoch [280/1000], Loss: 0.048208\n",
      "Epoch [290/1000], Loss: 0.047938\n",
      "Epoch [300/1000], Loss: 0.047674\n",
      "Epoch [310/1000], Loss: 0.047416\n",
      "Epoch [320/1000], Loss: 0.047163\n",
      "Epoch [330/1000], Loss: 0.046916\n",
      "Epoch [340/1000], Loss: 0.046674\n",
      "Epoch [350/1000], Loss: 0.046436\n",
      "Epoch [360/1000], Loss: 0.046204\n",
      "Epoch [370/1000], Loss: 0.045975\n",
      "Epoch [380/1000], Loss: 0.045752\n",
      "Epoch [390/1000], Loss: 0.045532\n",
      "Epoch [400/1000], Loss: 0.045316\n",
      "Epoch [410/1000], Loss: 0.045104\n",
      "Epoch [420/1000], Loss: 0.044896\n",
      "Epoch [430/1000], Loss: 0.044691\n",
      "Epoch [440/1000], Loss: 0.044490\n",
      "Epoch [450/1000], Loss: 0.044292\n",
      "Epoch [460/1000], Loss: 0.044098\n",
      "Epoch [470/1000], Loss: 0.043906\n",
      "Epoch [480/1000], Loss: 0.043717\n",
      "Epoch [490/1000], Loss: 0.043531\n",
      "Epoch [500/1000], Loss: 0.043348\n",
      "Epoch [510/1000], Loss: 0.043168\n",
      "Epoch [520/1000], Loss: 0.042990\n",
      "Epoch [530/1000], Loss: 0.042815\n",
      "Epoch [540/1000], Loss: 0.042642\n",
      "Epoch [550/1000], Loss: 0.042472\n",
      "Epoch [560/1000], Loss: 0.042304\n",
      "Epoch [570/1000], Loss: 0.042138\n",
      "Epoch [580/1000], Loss: 0.041975\n",
      "Epoch [590/1000], Loss: 0.041814\n",
      "Epoch [600/1000], Loss: 0.041655\n",
      "Epoch [610/1000], Loss: 0.041498\n",
      "Epoch [620/1000], Loss: 0.041343\n",
      "Epoch [630/1000], Loss: 0.041190\n",
      "Epoch [640/1000], Loss: 0.041040\n",
      "Epoch [650/1000], Loss: 0.040891\n",
      "Epoch [660/1000], Loss: 0.040744\n",
      "Epoch [670/1000], Loss: 0.040600\n",
      "Epoch [680/1000], Loss: 0.040457\n",
      "Epoch [690/1000], Loss: 0.040316\n",
      "Epoch [700/1000], Loss: 0.040178\n",
      "Epoch [710/1000], Loss: 0.040041\n",
      "Epoch [720/1000], Loss: 0.039906\n",
      "Epoch [730/1000], Loss: 0.039772\n",
      "Epoch [740/1000], Loss: 0.039641\n",
      "Epoch [750/1000], Loss: 0.039512\n",
      "Epoch [760/1000], Loss: 0.039384\n",
      "Epoch [770/1000], Loss: 0.039258\n",
      "Epoch [780/1000], Loss: 0.039134\n",
      "Epoch [790/1000], Loss: 0.039012\n",
      "Epoch [800/1000], Loss: 0.038891\n",
      "Epoch [810/1000], Loss: 0.038773\n",
      "Epoch [820/1000], Loss: 0.038656\n",
      "Epoch [830/1000], Loss: 0.038540\n",
      "Epoch [840/1000], Loss: 0.038427\n",
      "Epoch [850/1000], Loss: 0.038315\n",
      "Epoch [860/1000], Loss: 0.038204\n",
      "Epoch [870/1000], Loss: 0.038096\n",
      "Epoch [880/1000], Loss: 0.037989\n",
      "Epoch [890/1000], Loss: 0.037884\n",
      "Epoch [900/1000], Loss: 0.037780\n",
      "Epoch [910/1000], Loss: 0.037678\n",
      "Epoch [920/1000], Loss: 0.037577\n",
      "Epoch [930/1000], Loss: 0.037478\n",
      "Epoch [940/1000], Loss: 0.037380\n",
      "Epoch [950/1000], Loss: 0.037284\n",
      "Epoch [960/1000], Loss: 0.037190\n",
      "Epoch [970/1000], Loss: 0.037097\n",
      "Epoch [980/1000], Loss: 0.037005\n",
      "Epoch [990/1000], Loss: 0.036915\n",
      "Epoch [1000/1000], Loss: 0.036826\n",
      "Training complted\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": "# window_size=30\n# graph_shift_operator = build_graph_matrix(industry_encodings, industry_mask, wiki_encodings, wiki_mask, device)\n# X_train, y_train, train_masks = prepare_data(\n#     eod_data=eod_data,\n#     masks=masks,\n#     base_price=price_prediction,\n#     device=device,\n#     window_size=window_size)\n\n# # Initialize model\n# model = GCNModel(\n#     layers_dim=[(num_features*window_size, 15), (15, 1)],\n#     num_companies=num_companies,\n#     S=graph_shift_operator,\n#     device=device,\n#     K=1,\n#     L=1\n# ).to(device)\n\n# # Training with masked loss\n# criterion = nn.MSELoss(reduction='none')  # Don't reduce yet, we'll apply masks\n# #criterion = nn.L1Loss(reduction='none')  # Don't reduce yet, we'll apply masks\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# epochs = 1000\n# for epoch in range(epochs):\n#     model.train()\n#     optimizer.zero_grad()\n\n#     # Forward pass\n#     #predictions = model(X_train, adjacency_matrix)  # [batch, companies, 1]\n#     predictions = model(X_train)  # [batch, companies, 1]\n\n\n#     # Calculate masked loss (only on valid samples)\n#     loss_per_sample = criterion(predictions, y_train)  # [batch, companies, 1]\n#     masked_loss = loss_per_sample * train_masks.unsqueeze(-1)  # Apply mask\n\n#     # Average loss over valid samples only\n#     num_valid = train_masks.sum() + 1e-8\n#     #loss = masked_loss[:,:,model.output_dim-1].sum() / num_valid # Loss only for prediction_horizon day in future (1 day)\n#     loss = masked_loss.sum() / num_valid # Loss for all days up to prediction_horizon\n\n#     # Backward pass\n#     loss.backward()\n#     optimizer.step()\n\n#     if (epoch + 1) % 10 == 0:\n#         print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')\n\n# print(\"Training complted\")",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aFoJLinAlKA",
    "outputId": "b17c36dc-6da8-4c46-f9f0-45428e42f499",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:47.869140Z",
     "iopub.status.idle": "2025-10-29T17:53:47.869353Z",
     "shell.execute_reply.started": "2025-10-29T17:53:47.869253Z",
     "shell.execute_reply": "2025-10-29T17:53:47.869262Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-29T18:27:43.250158Z",
     "start_time": "2025-10-29T18:27:43.248324Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  }
 ]
}
