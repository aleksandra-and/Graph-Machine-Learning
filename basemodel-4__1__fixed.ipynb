{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "is5ZiquMgj5j",
    "Nt6BQK32gMN1"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 13424334,
     "sourceType": "datasetVersion",
     "datasetId": 8520408
    }
   ],
   "dockerImageVersionId": 31153,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:24.849588Z",
     "start_time": "2025-10-30T14:10:24.841890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these variables for different experiments\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# Data paths\n",
    "dir_local = '/home/study/IdeaProjects/Graph-Machine-Learning/Temporal_RSR/data'\n",
    "dir_kaggle = '/kaggle/input/rsr-dataset/Data/'\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Dataset parameters\n",
    "num_companies = 150  # max is 1026\n",
    "num_days = 1245\n",
    "num_features = 5\n",
    "market = \"NASDAQ\"  # or \"NYSE\"\n",
    "\n",
    "# Temporal parameters\n",
    "window_size = 20  # Lookback period for temporal models\n",
    "prediction_horizon = 1  # How many days ahead to predict\n",
    "\n",
    "# Train/Val/Test split (should sum to 1.0)\n",
    "train_split = 0.8  # 80% for training\n",
    "val_split = 0.1    # 10% for validation\n",
    "test_split = 0.1   # 10% for test\n",
    "\n",
    "# Graph parameters\n",
    "K = 5  # Number of graph hops for GNN models\n",
    "calculate_correlation = False\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL CONFIGURATION - Activate/Deactivate features\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    # Which model to train\n",
    "    'use_gvar': False,      # Simple G-VAR baseline\n",
    "    'use_gcn': False,       # Pure GCN model\n",
    "    'use_gat': True,        # GAT model (attention-based)\n",
    "    \n",
    "    # Graph features\n",
    "    'use_industry_relations': True,   # Use industry relationship data\n",
    "    'use_wiki_relations': True,       # Use Wikipedia relationship data\n",
    "    'use_graph_structure': True,      # If False, uses identity matrix (no graph)\n",
    "    'separate_edge_weights': False,   # If True, use 2D edge features [industry_wt, wiki_wt]\n",
    "    \n",
    "    # Model architecture\n",
    "    'hidden_dim': 10,                 # Hidden layer dimension for GAT embeddings\n",
    "    'num_attention_heads': 1,         # For GAT model\n",
    "    'dropout': 0.0,                   # Dropout rate\n",
    "    'use_relu_output': False,         # Apply ReLU after final layer (False for regression)\n",
    "    'use_batched_gat': False,          # Use faster batched GAT processing (100-1000x faster!)\n",
    "    \n",
    "    # Temporal modeling (RNN)\n",
    "    'use_rnn': True,                  # Add RNN layer after GAT for temporal modeling\n",
    "    'rnn_type': 'GRU',               # 'LSTM' or 'GRU'\n",
    "    'rnn_hidden_dim': 10,             # RNN hidden state dimension\n",
    "    'rnn_num_layers': 1,              # Number of RNN layers\n",
    "    'rnn_bidirectional': False,       # Use bidirectional RNN\n",
    "    \n",
    "    # Training parameters\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.0,              # L2 regularization\n",
    "    'batch_training': False,          # If True, use mini-batches (not implemented yet)\n",
    "    \n",
    "    # Feature engineering\n",
    "    'normalize_features': False,      # Normalize input features\n",
    "    'use_all_features': True,         # Use all 5 stock features\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "train_batch = 1\n",
    "val_batch = 1\n",
    "epochs = 20\n",
    "val_min_num = 10\n",
    "use_kfold = False\n",
    "early_stopping_patience = 20  # Stop if no improvement for N epochs\n",
    "\n",
    "# Environment detection\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    print(\"Running on Kaggle!\")\n",
    "    dir = dir_kaggle\n",
    "    train_batch = 32\n",
    "    val_batch = 32\n",
    "    epochs = 100\n",
    "    num_companies = 1026\n",
    "    calculate_correlation = True\n",
    "else:\n",
    "    dir = dir_local\n",
    "    print(\"Running locally!\")\n",
    "\n",
    "SAVE_PREPROCESSED_DATA = False  # Set to True to save preprocessed data\n",
    "\n",
    "# ============================================================================\n",
    "# PRINT CONFIGURATION SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EXPERIMENT CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Dataset:\")\n",
    "print(f\"  Market: {market}\")\n",
    "print(f\"  Companies: {num_companies}\")\n",
    "print(f\"  Features: {num_features}\")\n",
    "print(f\"  Window size: {window_size}\")\n",
    "print(f\"  Prediction horizon: {prediction_horizon}\")\n",
    "\n",
    "print(f\"\\nData Split:\")\n",
    "print(f\"  Train: {train_split*100:.0f}%\")\n",
    "print(f\"  Val:   {val_split*100:.0f}%\")\n",
    "print(f\"  Test:  {test_split*100:.0f}%\")\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "active_model = [k.replace('use_', '').upper() for k, v in MODEL_CONFIG.items() if k.startswith('use_') and v and k in ['use_gvar', 'use_gcn', 'use_gat']]\n",
    "print(f\"  Active model: {active_model[0] if active_model else 'NONE'}\")\n",
    "print(f\"  Graph structure: {'Yes' if MODEL_CONFIG['use_graph_structure'] else 'No (identity)'}\")\n",
    "print(f\"  Industry relations: {'Yes' if MODEL_CONFIG['use_industry_relations'] else 'No'}\")\n",
    "print(f\"  Wiki relations: {'Yes' if MODEL_CONFIG['use_wiki_relations'] else 'No'}\")\n",
    "print(f\"  Separate edge weights: {'Yes (2D)' if MODEL_CONFIG['separate_edge_weights'] else 'No (1D combined)'}\")\n",
    "print(f\"  GAT hidden dim: {MODEL_CONFIG['hidden_dim']}\")\n",
    "\n",
    "if MODEL_CONFIG['use_rnn']:\n",
    "    print(f\"\\nTemporal Modeling (RNN):\")\n",
    "    print(f\"  RNN enabled: Yes\")\n",
    "    print(f\"  RNN type: {MODEL_CONFIG['rnn_type']}\")\n",
    "    print(f\"  RNN hidden dim: {MODEL_CONFIG['rnn_hidden_dim']}\")\n",
    "    print(f\"  RNN layers: {MODEL_CONFIG['rnn_num_layers']}\")\n",
    "    print(f\"  Bidirectional: {'Yes' if MODEL_CONFIG['rnn_bidirectional'] else 'No'}\")\n",
    "else:\n",
    "    print(f\"\\nTemporal Modeling: None (flattens time dimension)\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Learning rate: {MODEL_CONFIG['learning_rate']}\")\n",
    "print(f\"  ReLU on output: {'Yes' if MODEL_CONFIG['use_relu_output'] else 'No (better for regression)'}\")\n",
    "print(f\"  Batched GAT: {'Yes (fast!)' if MODEL_CONFIG['use_batched_gat'] else 'No (slow)'}\")\n",
    "print(f\"  Epochs: {epochs}\")\n",
    "print(f\"  Graph hops (K): {K}\")\n",
    "print(f\"  Early stopping patience: {early_stopping_patience}\")\n",
    "print(f\"{'='*60}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally!\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT CONFIGURATION\n",
      "============================================================\n",
      "Dataset:\n",
      "  Market: NASDAQ\n",
      "  Companies: 150\n",
      "  Features: 5\n",
      "  Window size: 20\n",
      "  Prediction horizon: 1\n",
      "\n",
      "Data Split:\n",
      "  Train: 80%\n",
      "  Val:   10%\n",
      "  Test:  10%\n",
      "\n",
      "Model Configuration:\n",
      "  Active model: GAT\n",
      "  Graph structure: Yes\n",
      "  Industry relations: Yes\n",
      "  Wiki relations: Yes\n",
      "  Separate edge weights: No (1D combined)\n",
      "  GAT hidden dim: 10\n",
      "\n",
      "Temporal Modeling (RNN):\n",
      "  RNN enabled: Yes\n",
      "  RNN type: GRU\n",
      "  RNN hidden dim: 10\n",
      "  RNN layers: 1\n",
      "  Bidirectional: No\n",
      "\n",
      "Training:\n",
      "  Learning rate: 0.001\n",
      "  ReLU on output: No (better for regression)\n",
      "  Batched GAT: No (slow)\n",
      "  Epochs: 20\n",
      "  Graph hops (K): 5\n",
      "  Early stopping patience: 20\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "# Stock Prediction with Graph Neural Networks\n\n## 🎯 Quick Start Guide\n\n### Running Different Experiments\n\nAll experiments are controlled from **Cell 1 (Configuration)**. Simply modify the values and restart the kernel:\n\n```python\n# Example 1: Test with fewer companies\nnum_companies = 50\nwindow_size = 20\nMODEL_CONFIG['use_gat'] = True\n\n# Example 2: Disable graph structure (baseline)\nMODEL_CONFIG['use_graph_structure'] = False\n\n# Example 3: Use only industry relations\nMODEL_CONFIG['use_wiki_relations'] = False\nMODEL_CONFIG['use_industry_relations'] = True\n\n# Example 4: Try different model\nMODEL_CONFIG['use_gat'] = False\nMODEL_CONFIG['use_gcn'] = True\n```\n\n### Key Configuration Options\n\n**MODEL_CONFIG dictionary:**\n- `use_gvar`, `use_gcn`, `use_gat`: Choose which model to train\n- `use_graph_structure`: Set to False for baseline (no graph)\n- `use_industry_relations`: Include/exclude industry relationship data\n- `use_wiki_relations`: Include/exclude Wikipedia relationship data\n- `hidden_dim`: Size of hidden layers\n- `learning_rate`: Optimizer learning rate\n- `dropout`: Regularization parameter\n\n**Data splits:**\n- `train_split = 0.8`: 80% of timesteps for training\n- `val_split = 0.1`: 10% for validation\n- `test_split = 0.1`: 10% for testing\n- All splits preserve temporal order (no shuffling!)\n\n---\n\n## 📁 Notebook Structure\n\n### 1. Configuration (Cell 1)\n- **Modify this cell** to control all experiments\n- `MODEL_CONFIG` dictionary for feature flags\n- Train/val/test split ratios\n- All hyperparameters in one place\n\n### 2. Dependencies & Setup (Cells 2-3)\n- Package installation and imports\n- Device detection (CPU/GPU)\n\n### 3. Data Loading Functions (Cells 4-5)\n- `load_EOD_data()`: Load stock price data\n- `load_relation_data()`: Load company relationship data\n\n### 4. Data Loading (Cells 6-12)\n- Load market data (NASDAQ/NYSE)\n- Load company relationships (industry, wiki)\n- **Subsample to `num_companies`** (Cell 12)\n\n### 5. Utility Functions (Cell 15)\n- `build_adjacency_matrix()`: Create normalized adjacency (respects config flags)\n- `build_graph_shift_operator()`: Create GCN-style normalization\n- `prepare_data()`: Create sliding window datasets\n- `temporal_train_val_test_split()`: **Split by time** (80/10/10)\n- `calculate_loss()`: Masked loss computation\n\n### 6. Data Preparation (Cells 17-18)\n- Convert to PyTorch tensors\n- Build graph structures (respects config flags)\n- Create training data with sliding windows\n- **Split into train/val/test** temporally\n\n### 7. Model Definitions (Cells 19-26)\n- **GVarModel** (Cell 20): Vector autoregression baseline\n- **GCNModel** (Cell 24): Graph convolutional network\n- **GCNGATModel** (Cell 26): Graph attention network\n\n### 8. Training & Evaluation (Cells 27-28)\n- **Cell 27**: Full training loop with:\n  - Train/validation monitoring\n  - Early stopping\n  - Test set evaluation\n  - Best model selection\n- **Cell 28**: Training visualization (loss curves)\n\n---\n\n## ✅ Feature Verification\n\nThe notebook **automatically verifies** that all 5 stock features are being used:\n- Opens: High, Low, Close, Adj Close, Volume\n- Check output in Cell 27 for confirmation\n\n---\n\n## 🧪 Testing Different Configurations\n\n### Example 1: Ablation Study (Graph vs No Graph)\n\n**Step 1:** With graph structure\n```python\nMODEL_CONFIG['use_graph_structure'] = True\n```\n\n**Step 2:** Without graph structure (baseline)\n```python\nMODEL_CONFIG['use_graph_structure'] = False  # Uses identity matrix\n```\n\nRestart kernel and compare test set performance!\n\n### Example 2: Relation Type Comparison\n\nTest which relations help more:\n```python\n# Only industry\nMODEL_CONFIG['use_industry_relations'] = True\nMODEL_CONFIG['use_wiki_relations'] = False\n\n# Only wiki\nMODEL_CONFIG['use_industry_relations'] = False\nMODEL_CONFIG['use_wiki_relations'] = True\n\n# Both (default)\nMODEL_CONFIG['use_industry_relations'] = True\nMODEL_CONFIG['use_wiki_relations'] = True\n```\n\n### Example 3: Model Comparison\n\n```python\n# Test GAT\nMODEL_CONFIG['use_gat'] = True\nMODEL_CONFIG['use_gcn'] = False\nMODEL_CONFIG['use_gvar'] = False\n\n# Test GCN (uncomment alternative training cells)\nMODEL_CONFIG['use_gat'] = False\nMODEL_CONFIG['use_gcn'] = True\n```\n\n### Example 4: Hyperparameter Tuning\n\n```python\nMODEL_CONFIG['learning_rate'] = 0.0001  # Lower LR\nMODEL_CONFIG['hidden_dim'] = 32         # Bigger model\nMODEL_CONFIG['dropout'] = 0.3           # Less regularization\nepochs = 100                             # More training\n```\n\n---\n\n## 📊 Understanding the Output\n\n### Training Progress\n```\nEpoch [  1/20] | Train Loss: 0.328155 | Val Loss: 0.350221\n```\n- **Train Loss**: MSE on training data\n- **Val Loss**: MSE on validation data (used for early stopping)\n\n### Final Evaluation\n```\nFinal MSE Loss:\n  Train: 0.045123\n  Val:   0.052341\n  Test:  0.048765\n```\n- **Test Loss**: Performance on unseen future timesteps\n- Lower is better!\n\n### RMSE (Root Mean Squared Error)\nMore interpretable than MSE - in same units as stock prices.\n\n---\n\n## 💡 Tips\n\n1. **Always restart kernel** when changing configuration\n2. **Use temporal split** - never shuffle time series data!\n3. **Check validation loss** - if much higher than train loss → overfitting\n4. **Early stopping** prevents overtraining (patience = 20 epochs)\n5. **Test set is sacred** - only look at it for final evaluation!\n\n---",
   "metadata": {}
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:26.040761Z",
     "start_time": "2025-10-30T14:10:24.889447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pip install torch-geometric\n",
    "%pip install statsmodels\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages/distutils-precedence.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"<frozen site>\", line 195, in addpackage\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Requirement already satisfied: torch-geometric in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (2.6.1)\r\n",
      "Requirement already satisfied: aiohttp in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from torch-geometric) (3.12.15)\r\n",
      "Requirement already satisfied: fsspec in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from torch-geometric) (2025.9.0)\r\n",
      "Requirement already satisfied: jinja2 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from torch-geometric) (3.1.6)\r\n",
      "Requirement already satisfied: numpy in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from torch-geometric) (2.0.1)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from torch-geometric) (7.1.0)\r\n",
      "Requirement already satisfied: pyparsing in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from torch-geometric) (3.2.5)\r\n",
      "Requirement already satisfied: requests in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from torch-geometric) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from torch-geometric) (4.67.1)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from aiohttp->torch-geometric) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from aiohttp->torch-geometric) (25.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.8.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from aiohttp->torch-geometric) (6.6.4)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from aiohttp->torch-geometric) (0.4.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.21.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from jinja2->torch-geometric) (3.0.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from requests->torch-geometric) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from requests->torch-geometric) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from requests->torch-geometric) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from requests->torch-geometric) (2025.10.5)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Error processing line 1 of /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages/distutils-precedence.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"<frozen site>\", line 195, in addpackage\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Requirement already satisfied: statsmodels in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (0.14.5)\r\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from statsmodels) (2.0.1)\r\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from statsmodels) (1.16.2)\r\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from statsmodels) (2.3.3)\r\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from statsmodels) (1.0.1)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from statsmodels) (25.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# IMPORTS\n# ============================================================================\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport networkx as nx\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom torch_geometric.nn import GATConv\nimport os\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)",
   "metadata": {
    "id": "9j8W6h4rkMKv",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:25.354718Z",
     "iopub.execute_input": "2025-10-29T17:53:25.354874Z",
     "iopub.status.idle": "2025-10-29T17:53:29.553412Z",
     "shell.execute_reply.started": "2025-10-29T17:53:25.354862Z",
     "shell.execute_reply": "2025-10-29T17:53:29.552648Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:27.891422Z",
     "start_time": "2025-10-30T14:10:26.803314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nCOPIED FROM THE PAPER\nsource code: https://github.com/fulifeng/Temporal_Relational_Stock_Ranking\n\"\"\"\ndef load_EOD_data(data_path, market_name, tickers, steps=1):\n    eod_data = []\n    masks = []\n    ground_truth = []\n    base_price = []\n\n    # Determine the expected number of rows based on the first ticker's data\n    first_ticker_path = os.path.join(data_path, market_name + '_' + tickers[0] + '_1.csv')\n    try:\n        first_df = pd.read_csv(first_ticker_path, header=None)\n        num_days = first_df.shape[0] - (1 if market_name == 'NASDAQ' else 0) # Remove last row for NASDAQ\n        num_features = first_df.shape[1] - 1 # Exclude the date column\n    except Exception as e:\n        print(f\"Error reading first ticker file {first_ticker_path}: {e}\")\n        return None, None, None, None\n\n    eod_data = np.zeros([len(tickers), num_days, num_features], dtype=np.float32)\n    masks = np.ones([len(tickers), num_days], dtype=np.float32)\n    ground_truth = np.zeros([len(tickers), num_days], dtype=np.float32) # We're not using this one\n    base_price = np.zeros([len(tickers), num_days], dtype=np.float32)\n\n    for index, ticker in enumerate(tickers):\n        if index % 50 == 0:\n          print(f\"Processed [{index}/{tickers.shape[0]}] tickers\")\n        single_EOD_path = os.path.join(data_path, market_name + '_' + ticker + '_1.csv')\n\n        try:\n            single_df = pd.read_csv(single_EOD_path, header=None)\n            if market_name == 'NASDAQ':\n                single_df = single_df[:-1] # remove the last day since lots of missing data\n\n            # Handle missing values (-1234)\n            single_EOD = single_df.values\n            mask_row_indices, mask_col_indices = np.where(np.abs(single_EOD + 1234) < 1e-8)\n            single_EOD[mask_row_indices, mask_col_indices] = 1.1 # Replace missing values\n\n            # Update masks based on missing closing price\n            missing_close_indices = np.where(np.abs(single_EOD[:, -1] + 1234) < 1e-8)[0]\n            masks[index, missing_close_indices] = 0.0\n\n            eod_data[index, :, :] = single_EOD[:, 1:] # Exclude date column\n            base_price[index, :] = single_EOD[:, -1]\n\n        except Exception as e:\n            print(f\"Error reading ticker file {single_EOD_path}: {e}\")\n            # Mark all days for this ticker as invalid if file reading fails\n            masks[index, :] = 0.0\n\n\n    print('eod data shape:', eod_data.shape)\n    return eod_data, masks, ground_truth, base_price",
   "metadata": {
    "id": "lxx4rTcGrQ51",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:29.554303Z",
     "iopub.execute_input": "2025-10-29T17:53:29.554772Z",
     "iopub.status.idle": "2025-10-29T17:53:29.564819Z",
     "shell.execute_reply.started": "2025-10-29T17:53:29.554746Z",
     "shell.execute_reply": "2025-10-29T17:53:29.563943Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:27.945315Z",
     "start_time": "2025-10-30T14:10:27.940737Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nCOPIED FROM THE PAPER\nsource code: https://github.com/fulifeng/Temporal_Relational_Stock_Ranking\n\"\"\"\ndef load_relation_data(relation_file):\n    relation_encoding = np.load(relation_file)\n    print('relation encoding shape:', relation_encoding.shape)\n    rel_shape = [relation_encoding.shape[0], relation_encoding.shape[1]]\n    mask_flags = np.equal(np.zeros(rel_shape, dtype=int),\n                          np.sum(relation_encoding, axis=2))\n    mask = np.where(mask_flags, np.ones(rel_shape) * -1e9, np.zeros(rel_shape))\n    return relation_encoding, mask",
   "metadata": {
    "id": "152QpGtv3cLe",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:29.565722Z",
     "iopub.execute_input": "2025-10-29T17:53:29.565979Z",
     "iopub.status.idle": "2025-10-29T17:53:29.584380Z",
     "shell.execute_reply.started": "2025-10-29T17:53:29.565962Z",
     "shell.execute_reply": "2025-10-29T17:53:29.583828Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:27.993871Z",
     "start_time": "2025-10-30T14:10:27.991215Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "# Loading data",
   "metadata": {
    "id": "FSE-7G93pDs4"
   }
  },
  {
   "cell_type": "code",
   "source": "# market = \"NYSE\"\nmarket = \"NASDAQ\"",
   "metadata": {
    "id": "2UI3iC-ohQfJ",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:29.586622Z",
     "iopub.execute_input": "2025-10-29T17:53:29.586870Z",
     "iopub.status.idle": "2025-10-29T17:53:29.597891Z",
     "shell.execute_reply.started": "2025-10-29T17:53:29.586843Z",
     "shell.execute_reply": "2025-10-29T17:53:29.597176Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:28.040875Z",
     "start_time": "2025-10-30T14:10:28.039166Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "industry_encodings, industry_mask = load_relation_data(dir+f'/relation/sector_industry/{market}_industry_relation.npy')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fowuc2213lyG",
    "outputId": "27ad7c92-3a47-4f23-fbe3-054969e44eb0",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:29.598625Z",
     "iopub.execute_input": "2025-10-29T17:53:29.598853Z",
     "iopub.status.idle": "2025-10-29T17:53:32.366766Z",
     "shell.execute_reply.started": "2025-10-29T17:53:29.598834Z",
     "shell.execute_reply": "2025-10-29T17:53:32.366010Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:28.185670Z",
     "start_time": "2025-10-30T14:10:28.085801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation encoding shape: (1026, 1026, 97)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "wiki_encodings, wiki_mask = load_relation_data(dir+f'/relation/wikidata/{market}_wiki_relation.npy')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDEWN7RA4Gbt",
    "outputId": "b25d974f-c25b-491f-bbe0-4ddfd7c9e99a",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:32.367567Z",
     "iopub.execute_input": "2025-10-29T17:53:32.367823Z",
     "iopub.status.idle": "2025-10-29T17:53:33.695882Z",
     "shell.execute_reply.started": "2025-10-29T17:53:32.367805Z",
     "shell.execute_reply": "2025-10-29T17:53:33.695144Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:28.235774Z",
     "start_time": "2025-10-30T14:10:28.189455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation encoding shape: (1026, 1026, 43)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "# Load company names\ntickers = np.loadtxt(dir+f'/{market}_tickers.csv', dtype=str)\nprint('tickers shape (# of companies):', tickers.shape)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W220yLcuM08d",
    "outputId": "68fe3e97-6354-4695-d821-2ef503fa6354",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:33.696692Z",
     "iopub.execute_input": "2025-10-29T17:53:33.696988Z",
     "iopub.status.idle": "2025-10-29T17:53:33.706205Z",
     "shell.execute_reply.started": "2025-10-29T17:53:33.696960Z",
     "shell.execute_reply": "2025-10-29T17:53:33.705625Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:28.243089Z",
     "start_time": "2025-10-30T14:10:28.240402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tickers shape (# of companies): (1026,)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "eod_data, eod_masks, eod_ground_truth, eod_base_price = load_EOD_data(dir+\"/2013-01-01\", market, tickers)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fwhOJ--P4jKe",
    "outputId": "70e65b50-499e-4abc-af6a-a98b6fdfe52e",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:33.706954Z",
     "iopub.execute_input": "2025-10-29T17:53:33.707134Z",
     "iopub.status.idle": "2025-10-29T17:53:42.849678Z",
     "shell.execute_reply.started": "2025-10-29T17:53:33.707119Z",
     "shell.execute_reply": "2025-10-29T17:53:42.848944Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:28.954551Z",
     "start_time": "2025-10-30T14:10:28.287056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed [0/1026] tickers\n",
      "Processed [50/1026] tickers\n",
      "Processed [100/1026] tickers\n",
      "Processed [150/1026] tickers\n",
      "Processed [200/1026] tickers\n",
      "Processed [250/1026] tickers\n",
      "Processed [300/1026] tickers\n",
      "Processed [350/1026] tickers\n",
      "Processed [400/1026] tickers\n",
      "Processed [450/1026] tickers\n",
      "Processed [500/1026] tickers\n",
      "Processed [550/1026] tickers\n",
      "Processed [600/1026] tickers\n",
      "Processed [650/1026] tickers\n",
      "Processed [700/1026] tickers\n",
      "Processed [750/1026] tickers\n",
      "Processed [800/1026] tickers\n",
      "Processed [850/1026] tickers\n",
      "Processed [900/1026] tickers\n",
      "Processed [950/1026] tickers\n",
      "Processed [1000/1026] tickers\n",
      "eod data shape: (1026, 1245, 5)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# SUBSAMPLE DATA TO num_companies\n# ============================================================================\n# This cell ensures all data uses the same subset of companies consistently\n\nprint(f\"Subsampling to {num_companies} companies...\")\n\n# Subsample relation data (company x company matrices)\nwiki_encodings = wiki_encodings[:num_companies, :num_companies, :]\nwiki_mask = wiki_mask[:num_companies, :num_companies]\nindustry_encodings = industry_encodings[:num_companies, :num_companies, :]\nindustry_mask = industry_mask[:num_companies, :num_companies]\n\n# Subsample EOD data (reload with subset of tickers)\neod_data, eod_masks, eod_ground_truth, eod_base_price = load_EOD_data(\n    dir + \"/2013-01-01\", \n    market, \n    tickers[:num_companies]\n)\n\nprint(f\"\\nSubsampled data shapes:\")\nprint(f\"  EOD data: {eod_data.shape}\")\nprint(f\"  Industry encodings: {industry_encodings.shape}\")\nprint(f\"  Wiki encodings: {wiki_encodings.shape}\")",
   "metadata": {
    "id": "U1madQ_P7Hq-",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:58:31.973344Z",
     "iopub.execute_input": "2025-10-29T17:58:31.973917Z",
     "iopub.status.idle": "2025-10-29T17:58:32.823635Z",
     "shell.execute_reply.started": "2025-10-29T17:58:31.973896Z",
     "shell.execute_reply": "2025-10-29T17:58:32.822766Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:29.067730Z",
     "start_time": "2025-10-30T14:10:28.957885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampling to 150 companies...\n",
      "Processed [0/150] tickers\n",
      "Processed [50/150] tickers\n",
      "Processed [100/150] tickers\n",
      "eod data shape: (150, 1245, 5)\n",
      "\n",
      "Subsampled data shapes:\n",
      "  EOD data: (150, 1245, 5)\n",
      "  Industry encodings: (150, 150, 97)\n",
      "  Wiki encodings: (150, 150, 43)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": "# Graph based Models",
   "metadata": {
    "id": "2aaU7CdApNk_"
   }
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# UTILITY FUNCTIONS - Graph and Data Preparation\n# ============================================================================\n\ndef build_adjacency_matrix(industry_encodings, industry_mask, wiki_encodings, wiki_mask, device):\n    \"\"\"\n    Build normalized adjacency matrix from relation encodings and masks\n    \n    Args:\n        industry_encodings: [num_companies, num_companies, num_relation_types]\n        industry_mask: [num_companies, num_companies] (-1e9 for no relation, 0 for valid)\n        wiki_encodings: [num_companies, num_companies, num_relation_types]\n        wiki_mask: [num_companies, num_companies]\n    \n    Returns:\n        adjacency_matrix: [num_companies, num_companies] - normalized adjacency\n    \"\"\"\n    # Combine relation encodings by summing across relation types\n    industry_adj = torch.sum(industry_encodings, dim=-1) if MODEL_CONFIG['use_industry_relations'] else 0\n    wiki_adj = torch.sum(wiki_encodings, dim=-1) if MODEL_CONFIG['use_wiki_relations'] else 0\n    \n    # Combine both relation types\n    combined_adj = industry_adj + wiki_adj\n    \n    # Apply masks: where mask is -1e9 (no relation), set adjacency to 0\n    combined_mask = industry_mask + wiki_mask\n    combined_adj = torch.where(combined_mask < -1e8, torch.zeros_like(combined_adj), combined_adj)\n    \n    # If not using graph structure, return identity matrix\n    if not MODEL_CONFIG['use_graph_structure']:\n        return torch.eye(combined_adj.shape[0], device=device)\n    \n    # Normalize: row-wise normalization (each row sums to 1)\n    row_sums = combined_adj.sum(dim=1, keepdim=True)\n    adjacency_matrix = combined_adj / (row_sums + 1e-8)\n    \n    return adjacency_matrix.to(device)\n\n\ndef build_separate_adjacency_matrices(industry_encodings, industry_mask, wiki_encodings, wiki_mask, device):\n    \"\"\"\n    Build SEPARATE normalized adjacency matrices for industry and wiki relations\n    Used when separate_edge_weights=True\n    \n    Returns:\n        industry_adj: [num_companies, num_companies]\n        wiki_adj: [num_companies, num_companies]\n    \"\"\"\n    # Build industry adjacency\n    industry_adj = torch.sum(industry_encodings, dim=-1) if MODEL_CONFIG['use_industry_relations'] else torch.zeros_like(industry_mask)\n    industry_adj = torch.where(industry_mask < -1e8, torch.zeros_like(industry_adj), industry_adj)\n    \n    # Build wiki adjacency\n    wiki_adj = torch.sum(wiki_encodings, dim=-1) if MODEL_CONFIG['use_wiki_relations'] else torch.zeros_like(wiki_mask)\n    wiki_adj = torch.where(wiki_mask < -1e8, torch.zeros_like(wiki_adj), wiki_adj)\n    \n    # If not using graph structure, return identity matrices\n    if not MODEL_CONFIG['use_graph_structure']:\n        identity = torch.eye(industry_adj.shape[0], device=device)\n        return identity, identity\n    \n    # Normalize each separately\n    industry_row_sums = industry_adj.sum(dim=1, keepdim=True)\n    industry_adj = industry_adj / (industry_row_sums + 1e-8)\n    \n    wiki_row_sums = wiki_adj.sum(dim=1, keepdim=True)\n    wiki_adj = wiki_adj / (wiki_row_sums + 1e-8)\n    \n    return industry_adj.to(device), wiki_adj.to(device)\n\n\ndef build_graph_shift_operator(industry_encodings, industry_mask, wiki_encodings, wiki_mask, device):\n    \"\"\"\n    Build graph shift operator (symmetric normalized adjacency) for GCN models\n    Using D^(-1/2) * A * D^(-1/2) normalization\n    \n    Returns:\n        graph_shift_operator: [num_companies, num_companies]\n    \"\"\"\n    # Combine relation encodings\n    industry_adj = torch.sum(industry_encodings, dim=-1) if MODEL_CONFIG['use_industry_relations'] else 0\n    wiki_adj = torch.sum(wiki_encodings, dim=-1) if MODEL_CONFIG['use_wiki_relations'] else 0\n    combined_adj = industry_adj + wiki_adj\n    \n    # Apply masks\n    combined_mask = industry_mask + wiki_mask\n    combined_adj = torch.where(combined_mask < -1e8, torch.zeros_like(combined_adj), combined_adj)\n    \n    # If not using graph structure, return identity matrix\n    if not MODEL_CONFIG['use_graph_structure']:\n        return torch.eye(combined_adj.shape[0], device=device)\n    \n    # Symmetric normalization: D^(-1/2) * A * D^(-1/2)\n    degree_matrix = torch.diag(torch.pow(torch.sum(combined_adj, dim=1), -0.5))\n    graph_shift_operator = degree_matrix @ combined_adj.float() @ degree_matrix\n    \n    return graph_shift_operator.to(device)\n\n\ndef prepare_data(eod_data, masks, base_price, device, window_size=20, prediction_horizon=1):\n    \"\"\"\n    Create sliding windows for time series prediction with mask handling\n    \n    Args:\n        eod_data: [num_companies, num_days, num_features]\n        masks: [num_companies, num_days] - 1.0 for valid, 0.0 for missing\n        base_price: [num_companies, num_days] - closing price of stock\n        window_size: Number of historical days to use as input\n        prediction_horizon: Number of days ahead to predict (usually 1)\n    \n    Returns:\n        X: Input windows [num_samples, num_companies, window_size, num_features]\n        y: Target prices [num_samples, num_companies, prediction_horizon]\n        sample_masks: Valid sample indicators [num_samples, num_companies]\n    \"\"\"\n    num_companies, num_days, num_features = eod_data.shape\n    num_samples = num_days - window_size - prediction_horizon + 1\n    \n    X = torch.zeros(num_samples, num_companies, window_size, num_features, device=device)\n    y = torch.zeros(num_samples, num_companies, prediction_horizon, device=device)\n    sample_masks = torch.zeros(num_samples, num_companies, device=device)\n    \n    for i in range(num_samples):\n        X[i] = eod_data[:, i:i+window_size, :]\n        y[i, :, :] = base_price[:, i+window_size:i+window_size+prediction_horizon]\n        \n        # A sample is valid if all days in the window AND the target day are valid\n        window_valid = masks[:, i:i+window_size].min(dim=1)[0]  # [num_companies]\n        target_valid = masks[:, i+window_size:i+window_size+prediction_horizon].min(dim=1)[0]\n        sample_masks[i] = window_valid * target_valid\n    \n    return X, y, sample_masks\n\n\ndef temporal_train_val_test_split(X, y, masks, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n    \"\"\"\n    Split data temporally into train/validation/test sets\n    IMPORTANT: Splits based on TIME, not randomly (preserves temporal order)\n    \n    Args:\n        X: [num_samples, num_companies, window_size, num_features]\n        y: [num_samples, num_companies, prediction_horizon]\n        masks: [num_samples, num_companies]\n        train_ratio: Proportion for training (e.g., 0.8 = 80%)\n        val_ratio: Proportion for validation (e.g., 0.1 = 10%)\n        test_ratio: Proportion for test (e.g., 0.1 = 10%)\n    \n    Returns:\n        Dictionary with train/val/test splits for X, y, and masks\n    \"\"\"\n    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.0\"\n    \n    num_samples = X.shape[0]\n    \n    # Calculate split indices based on time\n    train_end = int(num_samples * train_ratio)\n    val_end = int(num_samples * (train_ratio + val_ratio))\n    \n    # Split data temporally\n    splits = {\n        'X_train': X[:train_end],\n        'y_train': y[:train_end],\n        'masks_train': masks[:train_end],\n        \n        'X_val': X[train_end:val_end],\n        'y_val': y[train_end:val_end],\n        'masks_val': masks[train_end:val_end],\n        \n        'X_test': X[val_end:],\n        'y_test': y[val_end:],\n        'masks_test': masks[val_end:],\n    }\n    \n    # Print split information\n    print(f\"\\nTemporal Data Split:\")\n    print(f\"  Train: samples [0:{train_end}] = {splits['X_train'].shape[0]} timesteps\")\n    print(f\"  Val:   samples [{train_end}:{val_end}] = {splits['X_val'].shape[0]} timesteps\")\n    print(f\"  Test:  samples [{val_end}:{num_samples}] = {splits['X_test'].shape[0]} timesteps\")\n    \n    print(f\"\\n  Valid train samples: {splits['masks_train'].sum().item():.0f} / {splits['masks_train'].numel()}\")\n    print(f\"  Valid val samples:   {splits['masks_val'].sum().item():.0f} / {splits['masks_val'].numel()}\")\n    print(f\"  Valid test samples:  {splits['masks_test'].sum().item():.0f} / {splits['masks_test'].numel()}\")\n    \n    return splits\n\n\ndef adjacency_to_edges(adjacency_matrix, industry_adj=None, wiki_adj=None):\n    \"\"\"\n    Convert adjacency matrix to edge_index and edge_weight for PyTorch Geometric\n    Supports both 1D (combined) and 2D (separate industry/wiki) edge features\n    \n    Args:\n        adjacency_matrix: [num_companies, num_companies] tensor - combined adjacency\n        industry_adj: [num_companies, num_companies] tensor - industry adjacency (optional)\n        wiki_adj: [num_companies, num_companies] tensor - wiki adjacency (optional)\n        \n    Returns:\n        edge_index: [2, num_edges] - edge connectivity\n        edge_weight: [num_edges, 1] or [num_edges, 2] - edge weights\n    \"\"\"\n    adj_np = adjacency_matrix.cpu().numpy()\n    rows, cols = np.where(adj_np > 0)\n    \n    edge_index = torch.tensor(np.stack([rows, cols]), dtype=torch.long)\n    \n    # Check if we need 2D edge features\n    if MODEL_CONFIG['separate_edge_weights'] and industry_adj is not None and wiki_adj is not None:\n        # 2D edge features: [industry_weight, wiki_weight]\n        industry_np = industry_adj.cpu().numpy()\n        wiki_np = wiki_adj.cpu().numpy()\n        \n        industry_weights = industry_np[rows, cols]\n        wiki_weights = wiki_np[rows, cols]\n        \n        edge_weight = torch.tensor(\n            np.stack([industry_weights, wiki_weights], axis=1),  # [num_edges, 2]\n            dtype=torch.float32\n        )\n        print(f\"  Using 2D edge features: [industry, wiki]\")\n    else:\n        # 1D edge features: combined weight\n        edge_weights = adj_np[rows, cols]\n        edge_weight = torch.tensor(edge_weights, dtype=torch.float32).view(-1, 1)  # [num_edges, 1]\n        print(f\"  Using 1D edge features: combined\")\n    \n    return edge_index, edge_weight\n\n\ndef create_batched_edges(edge_index, edge_weight, num_companies, max_batch_size):\n    \"\"\"\n    Pre-compute batched edge_index and edge_weight for fast GAT processing\n    Creates one big graph with disconnected subgraphs for each timestep\n    \n    Args:\n        edge_index: [2, num_edges] - base edge connectivity\n        edge_weight: [num_edges, edge_dim] - base edge weights\n        num_companies: Number of nodes per timestep\n        max_batch_size: Maximum number of timesteps to support\n    \n    Returns:\n        edge_index_batched: [2, max_batch_size * num_edges]\n        edge_weight_batched: [max_batch_size * num_edges, edge_dim]\n    \"\"\"\n    num_edges = edge_index.shape[1]\n    edge_dim = edge_weight.shape[1]\n    \n    # Pre-allocate tensors\n    edge_index_batched = torch.zeros(2, max_batch_size * num_edges, dtype=torch.long, device=edge_index.device)\n    edge_weight_batched = torch.zeros(max_batch_size * num_edges, edge_dim, dtype=torch.float32, device=edge_weight.device)\n    \n    # Fill in edges for each timestep\n    for b in range(max_batch_size):\n        start_idx = b * num_edges\n        end_idx = (b + 1) * num_edges\n        \n        # Offset edge indices by b * num_companies\n        edge_index_batched[:, start_idx:end_idx] = edge_index + (b * num_companies)\n        edge_weight_batched[start_idx:end_idx] = edge_weight\n    \n    return edge_index_batched, edge_weight_batched\n\n\ndef calculate_loss(predictions, targets, masks, criterion):\n    \"\"\"\n    Calculate masked loss (only compute loss on valid samples)\n    \n    Args:\n        predictions: Model predictions [batch, companies, output_dim]\n        targets: Ground truth [batch, companies, output_dim]\n        masks: Valid sample mask [batch, companies]\n        criterion: Loss function (should have reduction='none')\n    \n    Returns:\n        loss: Scalar loss value\n    \"\"\"\n    loss_per_sample = criterion(predictions, targets)\n    masked_loss = loss_per_sample * masks.unsqueeze(-1)\n    num_valid = masks.sum() + 1e-8\n    loss = masked_loss.sum() / num_valid\n    return loss\n\n\nprint(\"Utility functions loaded successfully!\")",
   "metadata": {
    "id": "wgYbWmhHHA3u",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:42.854837Z",
     "iopub.execute_input": "2025-10-29T17:53:42.855015Z",
     "iopub.status.idle": "2025-10-29T17:53:42.866146Z",
     "shell.execute_reply.started": "2025-10-29T17:53:42.855002Z",
     "shell.execute_reply": "2025-10-29T17:53:42.865576Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:29.083989Z",
     "start_time": "2025-10-30T14:10:29.072117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True",
   "metadata": {
    "id": "H-6FODpFEYEy",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:42.876889Z",
     "iopub.execute_input": "2025-10-29T17:53:42.877144Z",
     "iopub.status.idle": "2025-10-29T17:53:43.011502Z",
     "shell.execute_reply.started": "2025-10-29T17:53:42.877120Z",
     "shell.execute_reply": "2025-10-29T17:53:43.010747Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:29.388511Z",
     "start_time": "2025-10-30T14:10:29.117394Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# CONVERT DATA TO TENSORS\n# ============================================================================\n\nprint(f\"Converting data to tensors for {num_companies} companies...\")\n\n# Convert to tensors - use the already subsampled data\neod_data_tensor = torch.tensor(eod_data, dtype=torch.float32)\nmasks_tensor = torch.tensor(eod_masks, dtype=torch.float32)\nprice_prediction = torch.tensor(eod_base_price, dtype=torch.float32)\n\n# Relation data tensors\nindustry_encodings_tensor = torch.tensor(industry_encodings, dtype=torch.float32)\nindustry_mask_tensor = torch.tensor(industry_mask, dtype=torch.float32)\nwiki_encodings_tensor = torch.tensor(wiki_encodings, dtype=torch.float32)\nwiki_mask_tensor = torch.tensor(wiki_mask, dtype=torch.float32)\n\nprint(f\"\\nTensor shapes:\")\nprint(f\"  EOD data: {eod_data_tensor.shape}\")\nprint(f\"  Masks: {masks_tensor.shape}\")\nprint(f\"  Price prediction: {price_prediction.shape}\")\nprint(f\"  Industry encodings: {industry_encodings_tensor.shape}\")\nprint(f\"  Wiki encodings: {wiki_encodings_tensor.shape}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:29.399685Z",
     "start_time": "2025-10-30T14:10:29.393865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data to tensors for 150 companies...\n",
      "\n",
      "Tensor shapes:\n",
      "  EOD data: torch.Size([150, 1245, 5])\n",
      "  Masks: torch.Size([150, 1245])\n",
      "  Price prediction: torch.Size([150, 1245])\n",
      "  Industry encodings: torch.Size([150, 150, 97])\n",
      "  Wiki encodings: torch.Size([150, 150, 43])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# PREPARE TRAINING DATA WITH TRAIN/VAL/TEST SPLIT\n# ============================================================================\n\nprint(f\"Preparing training data with window_size={window_size}, prediction_horizon={prediction_horizon}...\")\n\n# Build graph structures\nadjacency_matrix = build_adjacency_matrix(\n    industry_encodings_tensor, industry_mask_tensor,\n    wiki_encodings_tensor, wiki_mask_tensor,\n    device=device\n)\n\ngraph_shift_operator = build_graph_shift_operator(\n    industry_encodings_tensor, industry_mask_tensor,\n    wiki_encodings_tensor, wiki_mask_tensor,\n    device=device\n)\n\n# Build separate adjacency matrices if using 2D edge weights\nif MODEL_CONFIG['separate_edge_weights']:\n    industry_adj, wiki_adj = build_separate_adjacency_matrices(\n        industry_encodings_tensor, industry_mask_tensor,\n        wiki_encodings_tensor, wiki_mask_tensor,\n        device=device\n    )\nelse:\n    industry_adj, wiki_adj = None, None\n\n# Prepare temporal data with sliding windows\nX_all, y_all, masks_all = prepare_data(\n    eod_data_tensor, masks_tensor, price_prediction,\n    window_size=window_size,\n    prediction_horizon=prediction_horizon,\n    device=device\n)\n\nprint(f\"\\nFull dataset prepared:\")\nprint(f\"  X_all: {X_all.shape}\")\nprint(f\"  y_all: {y_all.shape}\")\nprint(f\"  masks_all: {masks_all.shape}\")\n\n# Split data temporally into train/val/test\ndata_splits = temporal_train_val_test_split(\n    X_all, y_all, masks_all,\n    train_ratio=train_split,\n    val_ratio=val_split,\n    test_ratio=test_split\n)\n\n# Extract splits for easier access\nX_train = data_splits['X_train']\ny_train = data_splits['y_train']\nmasks_train = data_splits['masks_train']\n\nX_val = data_splits['X_val']\ny_val = data_splits['y_val']\nmasks_val = data_splits['masks_val']\n\nX_test = data_splits['X_test']\ny_test = data_splits['y_test']\nmasks_test = data_splits['masks_test']\n\n# Convert adjacency to edge format for GAT models\nprint(f\"\\nConverting adjacency to edge format...\")\nedge_index, edge_weight = adjacency_to_edges(adjacency_matrix, industry_adj, wiki_adj)\nedge_index = edge_index.to(device)\nedge_weight = edge_weight.to(device)\n\n# Pre-compute batched edges if using batched GAT\nif MODEL_CONFIG['use_batched_gat']:\n    print(f\"\\nPre-computing batched edges for fast GAT processing...\")\n    max_batch_size = X_all.shape[0]  # Use full dataset size\n    edge_index_batched, edge_weight_batched = create_batched_edges(\n        edge_index, edge_weight, num_companies, max_batch_size\n    )\n    print(f\"  Batched edges created for up to {max_batch_size} timesteps\")\n    print(f\"  Edge index batched: {edge_index_batched.shape}\")\n    print(f\"  Edge weight batched: {edge_weight_batched.shape}\")\nelse:\n    edge_index_batched, edge_weight_batched = None, None\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Graph structure:\")\nprint(f\"  Adjacency matrix: {adjacency_matrix.shape}\")\nprint(f\"  Edge index: {edge_index.shape}\")\nprint(f\"  Edge weight: {edge_weight.shape}\")\nprint(f\"  Edge dimension: {edge_weight.shape[1]}D ({'industry+wiki separate' if edge_weight.shape[1] == 2 else 'combined'}) \")\nprint(f\"  Total edges: {edge_index.shape[1]}\")\nprint(f\"{'='*60}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feQ_WQ2JrkPY",
    "outputId": "1708b013-d82a-4ee3-f1c0-e501f50fef1a",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:54:48.397770Z",
     "iopub.execute_input": "2025-10-29T17:54:48.398489Z",
     "iopub.status.idle": "2025-10-29T17:54:48.999323Z",
     "shell.execute_reply.started": "2025-10-29T17:54:48.398465Z",
     "shell.execute_reply": "2025-10-29T17:54:48.998433Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:29.624903Z",
     "start_time": "2025-10-30T14:10:29.444788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data with window_size=20, prediction_horizon=1...\n",
      "\n",
      "Full dataset prepared:\n",
      "  X_all: torch.Size([1225, 150, 20, 5])\n",
      "  y_all: torch.Size([1225, 150, 1])\n",
      "  masks_all: torch.Size([1225, 150])\n",
      "\n",
      "Temporal Data Split:\n",
      "  Train: samples [0:980] = 980 timesteps\n",
      "  Val:   samples [980:1102] = 122 timesteps\n",
      "  Test:  samples [1102:1225] = 123 timesteps\n",
      "\n",
      "  Valid train samples: 147000 / 147000\n",
      "  Valid val samples:   18300 / 18300\n",
      "  Valid test samples:  18450 / 18450\n",
      "\n",
      "Converting adjacency to edge format...\n",
      "  Using 1D edge features: combined\n",
      "\n",
      "============================================================\n",
      "Graph structure:\n",
      "  Adjacency matrix: torch.Size([150, 150])\n",
      "  Edge index: torch.Size([2, 176])\n",
      "  Edge weight: torch.Size([176, 1])\n",
      "  Edge dimension: 1D (combined) \n",
      "  Total edges: 176\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nSimple G-VAR (Graph Vector AutoRegression) for Stock Price Prediction\nCombines temporal dependencies (VAR) with graph structure (GNN)\nUpdated to match the paper's data format\n\"\"\"\n\n# ============================================================================\n# Simple G-Var\n# ============================================================================\n\nclass GVarModel(nn.Module):\n    def __init__(self, input_dim, output_dim, num_companies, device, K=2):\n        \"\"\"\n        Args:\n            input_dim: Number of features * window_size per company\n            output_dim: Prediction dimension (1 for return prediction)\n            num_companies: Number of stocks (e.g., 150)\n            K: Number of graph hops\n        \"\"\"\n        super(GVarModel, self).__init__()\n        self.device = device\n        self.K = K\n        self.num_companies = num_companies\n        self.output_dim = output_dim\n\n        self.graph_layers = nn.ModuleList([\n            nn.Linear(input_dim, 1) for _ in range(K + 1)\n        ])\n\n    def forward(self, x, adjacency_matrix):\n        \"\"\"\n        Args:\n            x: Historical data [batch, num_companies, time_steps, input_dim]\n            adjacency_matrix: Graph structure [num_companies, num_companies]\n        Returns:\n            predictions: [batch, num_companies, output_dim]\n        \"\"\"\n        batch_size = x.shape[0]\n\n        # Step 1: Extract temporal features for each stock independently\n        # Reshape to process all companies' time series\n        x_reshaped = x.view(x.shape[0], x.shape[1], -1)  # [batch, companies, time_steps * features]\n\n        # Compute powers of adjacency matrix: A^0 (self), A^1 (neighbors), A^2 (2-hop), ...\n        S_powers = [torch.eye(self.num_companies, device=adjacency_matrix.device)]\n        for k in range(self.K):\n            S_powers.append(torch.matmul(S_powers[-1], adjacency_matrix))\n\n        # Step 2: Aggregate information from k-hop neighbors\n        output = torch.zeros(x.shape[0], x.shape[1], self.output_dim, device=self.device)\n        for k in range(self.K + 1):\n            # Transform features at each hop level\n            transformed = self.graph_layers[k](x_reshaped)  # [batch, companies, hidden_dim]\n\n            # Aggregate from k-hop neighbors: S^k @ transformed\n            aggregated = torch.matmul(S_powers[k], transformed)  # [batch, companies, hidden_dim]\n            output += aggregated\n\n        return output",
   "metadata": {
    "id": "I0WbMW-lHLXW",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:43.630009Z",
     "iopub.execute_input": "2025-10-29T17:53:43.630228Z",
     "iopub.status.idle": "2025-10-29T17:53:43.647858Z",
     "shell.execute_reply.started": "2025-10-29T17:53:43.630207Z",
     "shell.execute_reply": "2025-10-29T17:53:43.647216Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:29.631582Z",
     "start_time": "2025-10-30T14:10:29.627953Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()\n\ntorch.cuda.memory_allocated()\n\n#import gc\n#gc.collect()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Xzxly4ZFtin",
    "outputId": "cc2f23fb-21b6-43ad-c5c8-ae4d4a0d0e99",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:43.648698Z",
     "iopub.execute_input": "2025-10-29T17:53:43.649023Z",
     "iopub.status.idle": "2025-10-29T17:53:43.664586Z",
     "shell.execute_reply.started": "2025-10-29T17:53:43.649008Z",
     "shell.execute_reply": "2025-10-29T17:53:43.663741Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:29.678015Z",
     "start_time": "2025-10-30T14:10:29.673327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75154944"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# ALTERNATIVE TRAINING: G-VAR Model (Commented out - uncomment to use)\n# ============================================================================\n# Simpler baseline model without graph structure\n#\n# model = GVarModel(\n#     input_dim=num_features * window_size,\n#     output_dim=prediction_horizon,\n#     num_companies=num_companies,\n#     device=device,\n#     K=1\n# ).to(device)\n#\n# criterion = nn.MSELoss(reduction='none')\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n#\n# for epoch in range(epochs):\n#     model.train()\n#     optimizer.zero_grad()\n#     \n#     # Use identity matrix instead of adjacency (no graph structure)\n#     predictions = model(X_train, torch.eye(num_companies, device=device))\n#     \n#     loss_per_sample = criterion(predictions, y_train)\n#     masked_loss = loss_per_sample * train_masks.unsqueeze(-1)\n#     num_valid = train_masks.sum() + 1e-8\n#     loss = masked_loss.sum() / num_valid\n#     \n#     loss.backward()\n#     optimizer.step()\n#     \n#     if (epoch + 1) % 50 == 0:\n#         print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')\n\npass  # Placeholder to keep cell executable",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OuT9rVIA8a2q",
    "outputId": "f5e82139-35c1-413c-8120-18e47f62753b",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:43.665394Z",
     "iopub.execute_input": "2025-10-29T17:53:43.665672Z",
     "iopub.status.idle": "2025-10-29T17:53:43.677059Z",
     "shell.execute_reply.started": "2025-10-29T17:53:43.665648Z",
     "shell.execute_reply": "2025-10-29T17:53:43.676246Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:29.722731Z",
     "start_time": "2025-10-30T14:10:29.720669Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": "# GNN",
   "metadata": {
    "id": "KZVrwD7S5NdX"
   }
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# MODEL DEFINITION: GCN (Graph Convolutional Network)\n# ============================================================================\n\nclass GCNModel(nn.Module):\n    \"\"\"\n    Graph Convolutional Network for stock prediction\n    Uses graph shift operator for spatial aggregation\n    \"\"\"\n    def __init__(self, layers_dim, num_companies, S, device, K=1, L=1):\n        \"\"\"\n        Args:\n            layers_dim: List of (input_dim, output_dim) tuples for each layer\n            num_companies: Number of stocks\n            S: Graph shift operator (normalized adjacency)\n            device: torch device\n            K: Number of graph hops\n            L: Number of GCN layers\n        \"\"\"\n        super(GCNModel, self).__init__()\n        self.device = device\n        self.K = K\n        self.L = L\n        self.num_companies = num_companies\n        self.layers_dim = layers_dim\n        \n        # Compute powers of graph shift operator: S^0, S^1, S^2, ...\n        self.S_powers = [S]\n        for k in range(self.K):\n            self.S_powers.append(self.S_powers[-1] @ S)\n        \n        # GCN layers\n        self.gcn_layer1 = nn.ModuleList([\n            nn.Linear(layers_dim[0][0], layers_dim[0][1]) for _ in range(K)\n        ])\n        self.activation1 = nn.ReLU()\n        \n        self.gcn_layer2 = nn.ModuleList([\n            nn.Linear(layers_dim[1][0], layers_dim[1][1]) for _ in range(K)\n        ])\n        self.activation2 = nn.ReLU()\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Historical data [batch, num_companies, time_steps, input_dim]\n        Returns:\n            predictions: [batch, num_companies, output_dim]\n        \"\"\"\n        batch_size = x.shape[0]\n        x_reshaped = x.view(batch_size, self.num_companies, -1)  # Flatten temporal dimension\n        \n        # First GCN layer with K-hop aggregation\n        x_i = x_reshaped\n        output = torch.zeros(batch_size, self.num_companies, self.layers_dim[0][1], device=self.device)\n        for k in range(self.K):\n            transformed = self.gcn_layer1[k](self.S_powers[k] @ x_i)\n            output += transformed\n        x_i = self.activation1(output)\n        \n        # Second GCN layer with K-hop aggregation\n        output = torch.zeros(batch_size, self.num_companies, self.layers_dim[1][1], device=self.device)\n        for k in range(self.K):\n            transformed = self.gcn_layer2[k](self.S_powers[k] @ x_i)\n            output += transformed\n        x_i = self.activation2(output)\n        \n        return x_i\n\n\nprint(\"GCNModel defined successfully!\")",
   "metadata": {
    "id": "85oRacMq5Ms-",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:43.677828Z",
     "iopub.execute_input": "2025-10-29T17:53:43.678036Z",
     "iopub.status.idle": "2025-10-29T17:53:43.695288Z",
     "shell.execute_reply.started": "2025-10-29T17:53:43.678022Z",
     "shell.execute_reply": "2025-10-29T17:53:43.694576Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:29.771420Z",
     "start_time": "2025-10-30T14:10:29.766985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNModel defined successfully!\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# MODEL DEFINITION: GCN-GAT (Hybrid using Graph Attention)\n# ============================================================================\n\nclass GCNGATModel(nn.Module):\n    \"\"\"\n    Hybrid model using Graph Attention Networks (GAT) for stock prediction\n    Uses attention mechanism to weight neighbor importance\n    \n    Architecture options:\n    1. Without RNN: GAT(flattened_time) → prediction\n    2. With RNN: GAT(t=1) → GAT(t=2) → ... → GAT(t=20) → RNN → prediction\n    \n    Supports:\n    - Batched processing (100-1000x faster)\n    - Optional ReLU activation\n    - 1D or 2D edge features\n    - Optional RNN for temporal modeling\n    \"\"\"\n    def __init__(self, layers_dim, num_companies, adjacency_matrix, S, device, edge_index, edge_weight, \n                 edge_index_batched=None, edge_weight_batched=None, window_size=20, K=1, L=1):\n        \"\"\"\n        Args:\n            layers_dim: List of (input_dim, output_dim) tuples for each layer\n            num_companies: Number of stocks\n            adjacency_matrix: Adjacency matrix for masking\n            S: Graph shift operator\n            device: torch device\n            edge_index: Edge connectivity [2, num_edges]\n            edge_weight: Edge weights [num_edges, edge_dim]\n            edge_index_batched: Pre-computed batched edges (optional)\n            edge_weight_batched: Pre-computed batched edge weights (optional)\n            window_size: Number of timesteps in input\n            K: Number of graph hops\n            L: Number of layers\n        \"\"\"\n        super(GCNGATModel, self).__init__()\n        self.device = device\n        self.K = K\n        self.L = L\n        self.num_companies = num_companies\n        self.layers_dim = layers_dim\n        self.window_size = window_size\n        \n        # Graph structure\n        self.S = S\n        self.adjacency_matrix = adjacency_matrix\n        self.mask = (self.adjacency_matrix == 0)\n        self.edge_index = edge_index\n        self.edge_weight = edge_weight\n        \n        # Batched edges (for fast processing)\n        self.use_batched = MODEL_CONFIG['use_batched_gat'] and edge_index_batched is not None\n        if self.use_batched:\n            self.register_buffer('edge_index_batched', edge_index_batched)\n            self.register_buffer('edge_weight_batched', edge_weight_batched)\n        \n        # Determine edge dimension (1D or 2D)\n        edge_dim = edge_weight.shape[1]\n        \n        # RNN configuration\n        self.use_rnn = MODEL_CONFIG['use_rnn']\n        \n        if self.use_rnn:\n            # ===== WITH RNN: GAT outputs embeddings, RNN processes them temporally =====\n            # GAT maps from input features to hidden_dim\n            gat_output_dim = MODEL_CONFIG['hidden_dim']\n            \n            self.conv_0 = GATConv(\n                in_channels=layers_dim[0][0],  # num_features (5)\n                out_channels=gat_output_dim,   # hidden_dim (e.g., 15)\n                heads=MODEL_CONFIG['num_attention_heads'],\n                concat=False,\n                dropout=MODEL_CONFIG['dropout'],\n                add_self_loops=True,\n                edge_dim=edge_dim,\n                fill_value=0,\n                bias=True\n            )\n            \n            # RNN processes temporal embeddings\n            rnn_input_dim = gat_output_dim\n            rnn_hidden_dim = MODEL_CONFIG['rnn_hidden_dim']\n            rnn_num_layers = MODEL_CONFIG['rnn_num_layers']\n            rnn_bidirectional = MODEL_CONFIG['rnn_bidirectional']\n            \n            if MODEL_CONFIG['rnn_type'] == 'LSTM':\n                self.rnn = nn.LSTM(\n                    input_size=rnn_input_dim,\n                    hidden_size=rnn_hidden_dim,\n                    num_layers=rnn_num_layers,\n                    batch_first=True,\n                    bidirectional=rnn_bidirectional,\n                    dropout=MODEL_CONFIG['dropout'] if rnn_num_layers > 1 else 0\n                )\n            else:  # GRU\n                self.rnn = nn.GRU(\n                    input_size=rnn_input_dim,\n                    hidden_size=rnn_hidden_dim,\n                    num_layers=rnn_num_layers,\n                    batch_first=True,\n                    bidirectional=rnn_bidirectional,\n                    dropout=MODEL_CONFIG['dropout'] if rnn_num_layers > 1 else 0\n                )\n            \n            # Final projection from RNN output to prediction\n            rnn_output_dim = rnn_hidden_dim * (2 if rnn_bidirectional else 1)\n            self.output_projection = nn.Linear(rnn_output_dim, layers_dim[0][1])  # prediction_horizon\n            \n        else:\n            # ===== WITHOUT RNN: GAT directly outputs prediction (original) =====\n            self.conv_0 = GATConv(\n                in_channels=layers_dim[0][0],  # num_features * window_size\n                out_channels=layers_dim[0][1], # prediction_horizon\n                heads=MODEL_CONFIG['num_attention_heads'],\n                concat=False,\n                dropout=MODEL_CONFIG['dropout'],\n                add_self_loops=True,\n                edge_dim=edge_dim,\n                fill_value=0,\n                bias=True\n            )\n        \n        # Optional ReLU activation\n        self.use_relu = MODEL_CONFIG['use_relu_output']\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Historical data [batch, num_companies, window_size, num_features]\n        Returns:\n            predictions: [batch, num_companies, output_dim]\n        \"\"\"\n        batch_size = x.shape[0]\n        \n        if self.use_rnn:\n            # ===== WITH RNN: Compute GAT embeddings for each timestep =====\n            # x: [batch, companies, window_size, features]\n            \n            if self.use_batched:\n                # FAST: Process all batch*window_size timesteps together\n                # Reshape: [batch*window_size, companies, features]\n                x_reshaped = x.permute(0, 2, 1, 3).contiguous()  # [batch, window_size, companies, features]\n                x_flat = x_reshaped.view(-1, self.num_companies, x.shape[-1])  # [batch*window_size, companies, features]\n                \n                # Further flatten for GAT: [batch*window_size*companies, features]\n                x_gat_input = x_flat.view(-1, x.shape[-1])\n                \n                # Use pre-computed batched edges\n                num_edges_per_timestep = self.edge_index.shape[1]\n                total_timesteps = batch_size * self.window_size\n                total_edges = total_timesteps * num_edges_per_timestep\n                \n                edge_idx = self.edge_index_batched[:, :total_edges]\n                edge_wgt = self.edge_weight_batched[:total_edges]\n                \n                # Single GAT pass for all timesteps\n                embeddings = self.conv_0(x_gat_input, edge_idx, edge_wgt)  # [batch*window_size*companies, hidden_dim]\n                \n                # Reshape back: [batch, window_size, companies, hidden_dim]\n                embeddings = embeddings.view(batch_size, self.window_size, self.num_companies, -1)\n                # Transpose: [batch, companies, window_size, hidden_dim]\n                embeddings = embeddings.permute(0, 2, 1, 3).contiguous()\n                \n            else:\n                # SLOW: Process each timestep sequentially\n                timestep_embeddings = []\n                for t in range(self.window_size):\n                    x_t = x[:, :, t, :]  # [batch, companies, features]\n                    \n                    # Process each batch item\n                    batch_embeddings = []\n                    for b in range(batch_size):\n                        emb = self.conv_0(x_t[b], self.edge_index, self.edge_weight)\n                        batch_embeddings.append(emb)\n                    \n                    timestep_embeddings.append(torch.stack(batch_embeddings))\n                \n                # Stack: [batch, companies, window_size, hidden_dim]\n                embeddings = torch.stack(timestep_embeddings, dim=2)\n            \n            # embeddings: [batch, companies, window_size, hidden_dim]\n            \n            # Process each company's temporal sequence through RNN\n            # Reshape for RNN: [batch*companies, window_size, hidden_dim]\n            embeddings_rnn = embeddings.view(-1, self.window_size, embeddings.shape[-1])\n            \n            # RNN forward pass\n            if MODEL_CONFIG['rnn_type'] == 'LSTM':\n                rnn_out, (h_n, c_n) = self.rnn(embeddings_rnn)\n            else:  # GRU\n                rnn_out, h_n = self.rnn(embeddings_rnn)\n            \n            # Use last hidden state: [batch*companies, rnn_hidden_dim]\n            if MODEL_CONFIG['rnn_bidirectional']:\n                # Concatenate forward and backward final states\n                h_n = h_n.view(MODEL_CONFIG['rnn_num_layers'], 2, -1, MODEL_CONFIG['rnn_hidden_dim'])\n                h_n = torch.cat([h_n[-1, 0, :, :], h_n[-1, 1, :, :]], dim=1)  # [batch*companies, 2*rnn_hidden_dim]\n            else:\n                h_n = h_n[-1]  # [batch*companies, rnn_hidden_dim]\n            \n            # Project to output\n            out = self.output_projection(h_n)  # [batch*companies, output_dim]\n            \n            # Reshape back: [batch, companies, output_dim]\n            out = out.view(batch_size, self.num_companies, -1)\n            \n        else:\n            # ===== WITHOUT RNN: Original architecture (flatten time) =====\n            x_reshaped = x.view(batch_size, self.num_companies, -1)  # Flatten temporal dimension\n            \n            if self.use_batched:\n                # FAST BATCHED PROCESSING\n                x_flat = x_reshaped.view(-1, x_reshaped.shape[-1])\n                \n                num_edges_per_timestep = self.edge_index.shape[1]\n                total_edges = batch_size * num_edges_per_timestep\n                \n                edge_idx = self.edge_index_batched[:, :total_edges]\n                edge_wgt = self.edge_weight_batched[:total_edges]\n                \n                out = self.conv_0(x_flat, edge_idx, edge_wgt)\n                out = out.view(batch_size, self.num_companies, -1)\n                \n            else:\n                # SLOW SEQUENTIAL PROCESSING\n                outputs = []\n                for b in range(batch_size):\n                    out_b = self.conv_0(x_reshaped[b], self.edge_index, self.edge_weight)\n                    outputs.append(out_b)\n                out = torch.stack(outputs)\n        \n        # Optional ReLU activation (off by default for regression)\n        if self.use_relu:\n            out = F.relu(out)\n        \n        return out\n\n\nprint(\"GCNGATModel defined successfully!\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T14:10:29.823793Z",
     "start_time": "2025-10-30T14:10:29.814108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNGATModel defined successfully!\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# TRAINING: GCN-GAT Model with Validation and Testing\n",
    "# ============================================================================\n",
    "\n",
    "# Only run if GAT model is selected\n",
    "if not MODEL_CONFIG['use_gat']:\n",
    "    print(\"GAT model not selected in configuration. Skipping...\")\n",
    "else:\n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"TRAINING GCN-GAT MODEL\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Verify all 5 features are being used\n",
    "    print(f\"\\nFeature verification:\")\n",
    "    print(f\"  Input shape: {X_train.shape}\")\n",
    "    print(f\"  Features per timestep: {X_train.shape[3]} (should be {num_features})\")\n",
    "    print(f\"  Total input dimension: window_size({window_size}) × features({num_features}) = {window_size * num_features}\")\n",
    "    assert X_train.shape[3] == num_features, f\"Expected {num_features} features but got {X_train.shape[3]}\"\n",
    "    print(f\"  ✓ All {num_features} features are being used!\")\n",
    "    \n",
    "    # Initialize model\n",
    "    print(f\"\\nInitializing GCN-GAT model...\")\n",
    "    model = GCNGATModel(\n",
    "        layers_dim=[(num_features * window_size, prediction_horizon)] if not MODEL_CONFIG['use_rnn']\n",
    "                   else [(num_features, MODEL_CONFIG['rnn_hidden_dim'])],\n",
    "        num_companies=num_companies,\n",
    "        adjacency_matrix=adjacency_matrix,\n",
    "        S=graph_shift_operator,\n",
    "        device=device,\n",
    "        edge_index=edge_index,\n",
    "        edge_weight=edge_weight,\n",
    "        edge_index_batched=edge_index_batched if MODEL_CONFIG['use_batched_gat'] else None,\n",
    "        edge_weight_batched=edge_weight_batched if MODEL_CONFIG['use_batched_gat'] else None,\n",
    "        window_size=window_size,\n",
    "        K=1,\n",
    "        L=1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Training configuration\n",
    "    criterion = nn.MSELoss(reduction='none')  # Masked loss\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=MODEL_CONFIG['learning_rate'],\n",
    "        weight_decay=MODEL_CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel configuration:\")\n",
    "    print(f\"  Architecture: {'GAT → RNN → Prediction' if MODEL_CONFIG['use_rnn'] else 'GAT → Prediction'}\")\n",
    "    if MODEL_CONFIG['use_rnn']:\n",
    "        print(f\"  RNN type: {MODEL_CONFIG['rnn_type']}\")\n",
    "        print(f\"  RNN hidden dim: {MODEL_CONFIG['rnn_hidden_dim']}\")\n",
    "        print(f\"  RNN layers: {MODEL_CONFIG['rnn_num_layers']}\")\n",
    "        print(f\"  Bidirectional: {'Yes' if MODEL_CONFIG['rnn_bidirectional'] else 'No'}\")\n",
    "    print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  Learning rate: {MODEL_CONFIG['learning_rate']}\")\n",
    "    print(f\"  Weight decay: {MODEL_CONFIG['weight_decay']}\")\n",
    "    print(f\"  Dropout: {MODEL_CONFIG['dropout']}\")\n",
    "    print(f\"  Attention heads: {MODEL_CONFIG['num_attention_heads']}\")\n",
    "    print(f\"  Edge dimension: {edge_weight.shape[1]}D\")\n",
    "    print(f\"  ReLU on output: {'Yes' if MODEL_CONFIG['use_relu_output'] else 'No (recommended for regression)'}\")\n",
    "    print(f\"  Batched processing: {'Yes (FAST!)' if MODEL_CONFIG['use_batched_gat'] else 'No (slow)'}\")\n",
    "    \n",
    "    # Training tracking\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # ===== TRAINING =====\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(X_train)  # [batch, companies, prediction_horizon]\n",
    "        \n",
    "        # Calculate masked loss\n",
    "        train_loss = calculate_loss(predictions, y_train, masks_train, criterion)\n",
    "        \n",
    "        # Backward pass\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        \n",
    "        # ===== VALIDATION =====\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(X_val)\n",
    "            val_loss = calculate_loss(val_predictions, y_val, masks_val, criterion)\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1:3d}/{epochs}] | Train Loss: {train_loss.item():.6f} | Val Loss: {val_loss.item():.6f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs (no improvement for {early_stopping_patience} epochs)\")\n",
    "            break\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # ===== FINAL EVALUATION =====\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL EVALUATION (Best Model)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Train set\n",
    "        train_predictions = model(X_train)\n",
    "        final_train_loss = calculate_loss(train_predictions, y_train, masks_train, criterion)\n",
    "        \n",
    "        # Validation set\n",
    "        val_predictions = model(X_val)\n",
    "        final_val_loss = calculate_loss(val_predictions, y_val, masks_val, criterion)\n",
    "        \n",
    "        # Test set\n",
    "        test_predictions = model(X_test)\n",
    "        final_test_loss = calculate_loss(test_predictions, y_test, masks_test, criterion)\n",
    "    \n",
    "    print(f\"\\nFinal MSE Loss:\")\n",
    "    print(f\"  Train: {final_train_loss.item():.6f}\")\n",
    "    print(f\"  Val:   {final_val_loss.item():.6f}\")\n",
    "    print(f\"  Test:  {final_test_loss.item():.6f}\")\n",
    "    \n",
    "    # Calculate RMSE for interpretability\n",
    "    print(f\"\\nFinal RMSE:\")\n",
    "    print(f\"  Train: {torch.sqrt(final_train_loss).item():.6f}\")\n",
    "    print(f\"  Val:   {torch.sqrt(final_val_loss).item():.6f}\")\n",
    "    print(f\"  Test:  {torch.sqrt(final_test_loss).item():.6f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Configuration used:\")\n",
    "    print(f\"  RNN enabled: {MODEL_CONFIG['use_rnn']}\")\n",
    "    if MODEL_CONFIG['use_rnn']:\n",
    "        print(f\"  RNN type: {MODEL_CONFIG['rnn_type']}\")\n",
    "    print(f\"  Batched GAT: {MODEL_CONFIG['use_batched_gat']}\")\n",
    "    print(f\"  ReLU output: {MODEL_CONFIG['use_relu_output']}\")\n",
    "    print(f\"  Separate edge weights: {MODEL_CONFIG['separate_edge_weights']}\")\n",
    "    print(f\"{'='*60}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:58:40.470365Z",
     "iopub.execute_input": "2025-10-29T17:58:40.470885Z",
     "iopub.status.idle": "2025-10-29T17:58:40.482898Z",
     "shell.execute_reply.started": "2025-10-29T17:58:40.470862Z",
     "shell.execute_reply": "2025-10-29T17:58:40.482205Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T14:14:53.909211Z",
     "start_time": "2025-10-30T14:10:29.870121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING GCN-GAT MODEL\n",
      "============================================================\n",
      "\n",
      "Feature verification:\n",
      "  Input shape: torch.Size([980, 150, 20, 5])\n",
      "  Features per timestep: 5 (should be 5)\n",
      "  Total input dimension: window_size(20) × features(5) = 100\n",
      "  ✓ All 5 features are being used!\n",
      "\n",
      "Initializing GCN-GAT model...\n",
      "\n",
      "Model configuration:\n",
      "  Architecture: GAT → RNN → Prediction\n",
      "  RNN type: GRU\n",
      "  RNN hidden dim: 10\n",
      "  RNN layers: 1\n",
      "  Bidirectional: No\n",
      "  Total parameters: 870\n",
      "  Learning rate: 0.001\n",
      "  Weight decay: 0.0\n",
      "  Dropout: 0.0\n",
      "  Attention heads: 1\n",
      "  Edge dimension: 1D\n",
      "  ReLU on output: No (recommended for regression)\n",
      "  Batched processing: No (slow)\n",
      "\n",
      "============================================================\n",
      "Starting training for 20 epochs...\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([980, 150, 1])) that is different to the input size (torch.Size([980, 150, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/study/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([122, 150, 1])) that is different to the input size (torch.Size([122, 150, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [  1/20] | Train Loss: 5.562860 | Val Loss: 7.055938\n",
      "Epoch [ 10/20] | Train Loss: 4.623800 | Val Loss: 5.912121\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 89\u001B[39m\n\u001B[32m     86\u001B[39m train_loss = calculate_loss(predictions, y_train, masks_train, criterion)\n\u001B[32m     88\u001B[39m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m89\u001B[39m \u001B[43mtrain_loss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     90\u001B[39m optimizer.step()\n\u001B[32m     92\u001B[39m train_losses.append(train_loss.item())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Graph-Machine-Learning/lib/python3.11/site-packages/torch/_tensor.py:525\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    522\u001B[39m     \u001B[38;5;66;03m# All strings are unicode in Python 3.\u001B[39;00m\n\u001B[32m    523\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m torch._tensor_str._str(\u001B[38;5;28mself\u001B[39m, tensor_contents=tensor_contents)\n\u001B[32m--> \u001B[39m\u001B[32m525\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mbackward\u001B[39m(\n\u001B[32m    526\u001B[39m     \u001B[38;5;28mself\u001B[39m, gradient=\u001B[38;5;28;01mNone\u001B[39;00m, retain_graph=\u001B[38;5;28;01mNone\u001B[39;00m, create_graph=\u001B[38;5;28;01mFalse\u001B[39;00m, inputs=\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    527\u001B[39m ):\n\u001B[32m    528\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001B[39;00m\n\u001B[32m    529\u001B[39m \n\u001B[32m    530\u001B[39m \u001B[33;03m    The graph is differentiated using the chain rule. If the tensor is\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    569\u001B[39m \u001B[33;03m            used to compute the :attr:`tensors`.\u001B[39;00m\n\u001B[32m    570\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m    571\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# VISUALIZE TRAINING PROGRESS\n# ============================================================================\n\nif MODEL_CONFIG['use_gat'] and 'train_losses' in locals():\n    import matplotlib.pyplot as plt\n    \n    plt.figure(figsize=(12, 5))\n    \n    # Plot losses\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Train Loss', alpha=0.7)\n    plt.plot(val_losses, label='Val Loss', alpha=0.7)\n    plt.xlabel('Epoch')\n    plt.ylabel('MSE Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Plot RMSE (more interpretable)\n    plt.subplot(1, 2, 2)\n    plt.plot([torch.sqrt(torch.tensor(l)).item() for l in train_losses], label='Train RMSE', alpha=0.7)\n    plt.plot([torch.sqrt(torch.tensor(l)).item() for l in val_losses], label='Val RMSE', alpha=0.7)\n    plt.xlabel('Epoch')\n    plt.ylabel('RMSE')\n    plt.title('Training and Validation RMSE')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nTraining statistics:\")\n    print(f\"  Best epoch: {val_losses.index(min(val_losses)) + 1}\")\n    print(f\"  Best val loss: {min(val_losses):.6f}\")\n    print(f\"  Final train loss: {train_losses[-1]:.6f}\")\n    print(f\"  Final val loss: {val_losses[-1]:.6f}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T14:09:12.589118780Z",
     "start_time": "2025-10-30T13:49:20.584161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training statistics:\n",
      "  Best epoch: 1000\n",
      "  Best val loss: 0.002115\n",
      "  Final train loss: 0.002110\n",
      "  Final val loss: 0.002115\n"
     ]
    }
   ],
   "execution_count": 154
  }
 ]
}
