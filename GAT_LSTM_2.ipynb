{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "is5ZiquMgj5j",
    "Nt6BQK32gMN1"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 13424334,
     "sourceType": "datasetVersion",
     "datasetId": 8520408
    }
   ],
   "dockerImageVersionId": 31153,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Data paths\n",
    "dir_local = '/home/study/IdeaProjects/Graph-Machine-Learning/Temporal_RSR/data'\n",
    "dir_kaggle = '/kaggle/input/rsr-dataset/Data/'\n",
    "\n",
    "num_companies = 1026  # max is 1026\n",
    "num_days = 1245\n",
    "num_features = 5\n",
    "market = \"NASDAQ\"  # or \"NYSE\"\n",
    "\n",
    "train_size = 756\n",
    "validation_size = 252\n",
    "test_size = 237\n",
    "\n",
    "# Temporal parameters\n",
    "window_size = 20  # Lookback period for temporal models\n",
    "prediction_horizon = 1  # How many days ahead to predict\n",
    "\n",
    "BASE_MODEL_CONFIG = {\n",
    "    'use_gat': True,        # GAT model (attention-based)\n",
    "    \n",
    "    # Graph features\n",
    "    'use_industry_relations': True,   # Use industry relationship data\n",
    "    'use_wiki_relations': True,       # Use Wikipedia relationship data\n",
    "    'use_graph_structure': True,      # If False, uses identity matrix (no graph)\n",
    "    'separate_edge_weights': True,    # If True, use 2D edge features [industry_wt, wiki_wt]\n",
    "    \n",
    "    # Model architecture\n",
    "    'hidden_dim': 16,                 # Hidden layer dimension for GAT embeddings\n",
    "    'num_attention_heads': 3,         # For GAT model\n",
    "    'dropout': 0.0,                   # Dropout rate\n",
    "    'use_relu_output': False,         # Apply ReLU after final layer (False for regression)\n",
    "    'use_graph_concatenation': True,   # Use graph concatenation, this allows for multiple graphs all processed together at once\n",
    "    \n",
    "    # Temporal modeling (RNN)\n",
    "    'use_rnn': False,                  # Add RNN layer after GAT for temporal modeling\n",
    "    'use_rnn_first': False,            # Use RNN before GAT layer, otherwise after\n",
    "    'rnn_type': 'LSTM',               # 'LSTM' or 'GRU'\n",
    "    'rnn_hidden_dim': 16,             # RNN hidden state dimension\n",
    "    'rnn_num_layers': 1,              # Number of RNN layers\n",
    "    \n",
    "    # Training parameters\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.0,              # L2 regularization\n",
    "    'batch_size': 4,                  # batch size for training\n",
    "}\n",
    "\n",
    "CONFIGS_TO_TEST = [\n",
    "    # Baseline: No temporal modeling\n",
    "    {'name': 'GAT_only', 'use_rnn': False},\n",
    "    \n",
    "    # RNN variants - GAT first then RNN\n",
    "    # {'name': 'GAT_LSTM', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'LSTM'},\n",
    "    # {'name': 'GAT_GRU', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'GRU'},\n",
    "\n",
    "    {'name': 'GAT_LSTM', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'LSTM'},\n",
    "\n",
    "    {'name': 'GAT_LSTM_attention_heads_1', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'LSTM', 'hidden_dim': 16, \"num_attention_heads\": 1},\n",
    "    {'name': 'GAT_LSTM_attention_heads_2', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'LSTM', 'hidden_dim': 16, \"num_attention_heads\": 2},\n",
    "    {'name': 'GAT_LSTM_attention_heads_3', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'LSTM', 'hidden_dim': 16, \"num_attention_heads\": 3},\n",
    "    {'name': 'GAT_LSTM_attention_heasd_4', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'LSTM', 'hidden_dim': 16, \"num_attention_heads\": 4},\n",
    "    # Different hidden dimensions\n",
    "    {'name': 'GAT_LSTM_h16', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'LSTM', 'hidden_dim': 16},\n",
    "    {'name': 'GAT_LSTM_h32', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'LSTM', 'hidden_dim': 32,},\n",
    "    # RNN variants - RNN first then GAT  \n",
    "    # {'name': 'LSTM_GAT', 'use_rnn': True, 'use_rnn_first': True, 'rnn_type': 'LSTM'},\n",
    "    # {'name': 'GRU_GAT', 'use_rnn': True, 'use_rnn_first': True, 'rnn_type': 'GRU'},\n",
    "\n",
    "    # Different hidden dimensions for LSTM\n",
    "    {'name': 'GAT_LSTM_h16', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'LSTM', 'hidden_dim': 16, 'rnn_hidden_dim': 16},\n",
    "    {'name': 'GAT_LSTM_h16', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'LSTM', 'hidden_dim': 16, 'rnn_hidden_dim': 32},\n",
    "    {'name': 'GAT_LSTM_h16', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'LSTM', 'hidden_dim': 16, 'rnn_hidden_dim': 64},\n",
    "    {'name': 'GAT_LSTM_h16', 'use_rnn': True, 'use_rnn_first': False, 'rnn_type': 'LSTM', 'hidden_dim': 16, 'rnn_hidden_dim': 128},\n",
    "    \n",
    "    # Different learning rates\n",
    "    {'name': 'GAT_LSTM_lr0.01', 'use_rnn': True, 'use_rnn_first': False, 'learning_rate': 0.01},\n",
    "    {'name': 'GAT_LSTM_lr0.0001', 'use_rnn': True, 'use_rnn_first': False, 'learning_rate': 0.0001},\n",
    "]\n",
    "\n",
    "# Working configuration (will be updated during grid search)\n",
    "MODEL_CONFIG = BASE_MODEL_CONFIG.copy()\n",
    "\n",
    "epochs = 1000\n",
    "early_stopping_patience = 100  # Stop if no improvement for N epochs\n",
    "the_max_batch_size = 32  # Max batch size that is used in configs\n",
    "\n",
    "# Environment detection\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    print(\"Running on Kaggle!\")\n",
    "    dir = dir_kaggle\n",
    "    epochs = 1000\n",
    "    num_companies = 1026\n",
    "else:\n",
    "    dir = dir_local\n",
    "    print(\"Running locally!\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"GRID SEARCH CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Dataset:\")\n",
    "print(f\"  Market: {market}\")\n",
    "print(f\"  Companies: {num_companies}\")\n",
    "print(f\"  Features: {num_features}\")\n",
    "print(f\"  Window size: {window_size}\")\n",
    "print(f\"  Prediction horizon: {prediction_horizon}\")\n",
    "\n",
    "print(f\"\\nData Split: default based on the one from Temporal Relational Stock Ranking paper\")\n",
    "print(f\"\\nGrid Search:\")\n",
    "print(f\"  Number of configurations: {len(CONFIGS_TO_TEST)}\")\n",
    "print(f\"  Configurations to test:\")\n",
    "for i, cfg in enumerate(CONFIGS_TO_TEST, 1):\n",
    "    print(f\"    {i}. {cfg['name']}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Epochs per config: {epochs}\")\n",
    "print(f\"  Early stopping patience: {early_stopping_patience}\")\n",
    "print(f\"{'='*60}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install torch-geometric\n",
    "%pip install statsmodels\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torch_geometric.nn import GATConv\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ],
   "metadata": {
    "id": "9j8W6h4rkMKv",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:25.354718Z",
     "iopub.execute_input": "2025-10-29T17:53:25.354874Z",
     "iopub.status.idle": "2025-10-29T17:53:29.553412Z",
     "shell.execute_reply.started": "2025-10-29T17:53:25.354862Z",
     "shell.execute_reply": "2025-10-29T17:53:29.552648Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nCOPIED FROM THE PAPER\nsource code: https://github.com/fulifeng/Temporal_Relational_Stock_Ranking\n\"\"\"\ndef load_EOD_data(data_path, market_name, tickers, steps=1):\n    eod_data = []\n    masks = []\n    ground_truth = []\n    base_price = []\n\n    # Determine the expected number of rows based on the first ticker's data\n    first_ticker_path = os.path.join(data_path, market_name + '_' + tickers[0] + '_1.csv')\n    try:\n        first_df = pd.read_csv(first_ticker_path, header=None)\n        num_days = first_df.shape[0] - (1 if market_name == 'NASDAQ' else 0) # Remove last row for NASDAQ\n        num_features = first_df.shape[1] - 1 # Exclude the date column\n    except Exception as e:\n        print(f\"Error reading first ticker file {first_ticker_path}: {e}\")\n        return None, None, None, None\n\n    eod_data = np.zeros([len(tickers), num_days, num_features], dtype=np.float32)\n    masks = np.ones([len(tickers), num_days], dtype=np.float32)\n    ground_truth = np.zeros([len(tickers), num_days], dtype=np.float32) # We're not using this one\n    base_price = np.zeros([len(tickers), num_days], dtype=np.float32)\n\n    for index, ticker in enumerate(tickers):\n        if index % 50 == 0:\n          print(f\"Processed [{index}/{tickers.shape[0]}] tickers\")\n        single_EOD_path = os.path.join(data_path, market_name + '_' + ticker + '_1.csv')\n\n        try:\n            single_df = pd.read_csv(single_EOD_path, header=None)\n            if market_name == 'NASDAQ':\n                single_df = single_df[:-1] # remove the last day since lots of missing data\n\n            # Handle missing values (-1234)\n            single_EOD = single_df.values\n            mask_row_indices, mask_col_indices = np.where(np.abs(single_EOD + 1234) < 1e-8)\n            single_EOD[mask_row_indices, mask_col_indices] = 1.1 # Replace missing values\n\n            # Update masks based on missing closing price\n            missing_close_indices = np.where(np.abs(single_EOD[:, -1] + 1234) < 1e-8)[0]\n            masks[index, missing_close_indices] = 0.0\n\n            eod_data[index, :, :] = single_EOD[:, 1:] # Exclude date column\n            base_price[index, :] = single_EOD[:, -1]\n\n        except Exception as e:\n            print(f\"Error reading ticker file {single_EOD_path}: {e}\")\n            # Mark all days for this ticker as invalid if file reading fails\n            masks[index, :] = 0.0\n\n\n    print('eod data shape:', eod_data.shape)\n    return eod_data, masks, ground_truth, base_price",
   "metadata": {
    "id": "lxx4rTcGrQ51",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:29.554303Z",
     "iopub.execute_input": "2025-10-29T17:53:29.554772Z",
     "iopub.status.idle": "2025-10-29T17:53:29.564819Z",
     "shell.execute_reply.started": "2025-10-29T17:53:29.554746Z",
     "shell.execute_reply": "2025-10-29T17:53:29.563943Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nCOPIED FROM THE PAPER\nsource code: https://github.com/fulifeng/Temporal_Relational_Stock_Ranking\n\"\"\"\ndef load_relation_data(relation_file):\n    relation_encoding = np.load(relation_file)\n    print('relation encoding shape:', relation_encoding.shape)\n    rel_shape = [relation_encoding.shape[0], relation_encoding.shape[1]]\n    mask_flags = np.equal(np.zeros(rel_shape, dtype=int),\n                          np.sum(relation_encoding, axis=2))\n    mask = np.where(mask_flags, np.ones(rel_shape) * -1e9, np.zeros(rel_shape))\n    return relation_encoding, mask",
   "metadata": {
    "id": "152QpGtv3cLe",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:29.565722Z",
     "iopub.execute_input": "2025-10-29T17:53:29.565979Z",
     "iopub.status.idle": "2025-10-29T17:53:29.584380Z",
     "shell.execute_reply.started": "2025-10-29T17:53:29.565962Z",
     "shell.execute_reply": "2025-10-29T17:53:29.583828Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Loading data",
   "metadata": {
    "id": "FSE-7G93pDs4"
   }
  },
  {
   "cell_type": "code",
   "source": "industry_encodings, industry_mask = load_relation_data(dir+f'/relation/sector_industry/{market}_industry_relation.npy')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fowuc2213lyG",
    "outputId": "27ad7c92-3a47-4f23-fbe3-054969e44eb0",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:29.598625Z",
     "iopub.execute_input": "2025-10-29T17:53:29.598853Z",
     "iopub.status.idle": "2025-10-29T17:53:32.366766Z",
     "shell.execute_reply.started": "2025-10-29T17:53:29.598834Z",
     "shell.execute_reply": "2025-10-29T17:53:32.366010Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "wiki_encodings, wiki_mask = load_relation_data(dir+f'/relation/wikidata/{market}_wiki_relation.npy')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDEWN7RA4Gbt",
    "outputId": "b25d974f-c25b-491f-bbe0-4ddfd7c9e99a",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:32.367567Z",
     "iopub.execute_input": "2025-10-29T17:53:32.367823Z",
     "iopub.status.idle": "2025-10-29T17:53:33.695882Z",
     "shell.execute_reply.started": "2025-10-29T17:53:32.367805Z",
     "shell.execute_reply": "2025-10-29T17:53:33.695144Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Load company names\ntickers = np.loadtxt(dir+f'/{market}_tickers.csv', dtype=str)\nprint('tickers shape (# of companies):', tickers.shape)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W220yLcuM08d",
    "outputId": "68fe3e97-6354-4695-d821-2ef503fa6354",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:33.696692Z",
     "iopub.execute_input": "2025-10-29T17:53:33.696988Z",
     "iopub.status.idle": "2025-10-29T17:53:33.706205Z",
     "shell.execute_reply.started": "2025-10-29T17:53:33.696960Z",
     "shell.execute_reply": "2025-10-29T17:53:33.705625Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Subsampling to {num_companies} companies...\")\n",
    "\n",
    "# Subsample relation data (company x company matrices)\n",
    "wiki_encodings = wiki_encodings[:num_companies, :num_companies, :]\n",
    "wiki_mask = wiki_mask[:num_companies, :num_companies]\n",
    "industry_encodings = industry_encodings[:num_companies, :num_companies, :]\n",
    "industry_mask = industry_mask[:num_companies, :num_companies]\n",
    "\n",
    "# Subsample EOD data (reload with subset of tickers)\n",
    "eod_data, eod_masks, eod_ground_truth, eod_base_price = load_EOD_data(\n",
    "    dir + \"/2013-01-01\", \n",
    "    market, \n",
    "    tickers[:num_companies]\n",
    ")\n",
    "\n",
    "print(f\"\\nSubsampled data shapes:\")\n",
    "print(f\"  EOD data: {eod_data.shape}\")\n",
    "print(f\"  Industry encodings: {industry_encodings.shape}\")\n",
    "print(f\"  Wiki encodings: {wiki_encodings.shape}\")"
   ],
   "metadata": {
    "id": "U1madQ_P7Hq-",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:58:31.973344Z",
     "iopub.execute_input": "2025-10-29T17:58:31.973917Z",
     "iopub.status.idle": "2025-10-29T17:58:32.823635Z",
     "shell.execute_reply.started": "2025-10-29T17:58:31.973896Z",
     "shell.execute_reply": "2025-10-29T17:58:32.822766Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Utils",
   "metadata": {
    "id": "2aaU7CdApNk_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_adjacency_matrix(industry_encodings, industry_mask, wiki_encodings, wiki_mask, device):\n",
    "    \"\"\"\n",
    "    Build normalized adjacency matrix from relation encodings and masks\n",
    "    \n",
    "    Args:\n",
    "        industry_encodings: [num_companies, num_companies, num_relation_types]\n",
    "        industry_mask: [num_companies, num_companies] (-1e9 for no relation, 0 for valid)\n",
    "        wiki_encodings: [num_companies, num_companies, num_relation_types]\n",
    "        wiki_mask: [num_companies, num_companies]\n",
    "    \n",
    "    Returns:\n",
    "        adjacency_matrix: [num_companies, num_companies] - normalized adjacency\n",
    "    \"\"\"\n",
    "    # Combine relation encodings by summing across relation types\n",
    "    industry_adj = torch.sum(industry_encodings, dim=-1) if MODEL_CONFIG['use_industry_relations'] else 0\n",
    "    wiki_adj = torch.sum(wiki_encodings, dim=-1) if MODEL_CONFIG['use_wiki_relations'] else 0\n",
    "    \n",
    "    # Combine both relation types\n",
    "    combined_adj = industry_adj + wiki_adj\n",
    "    \n",
    "    # Apply masks: where mask is -1e9 (no relation), set adjacency to 0\n",
    "    combined_mask = industry_mask + wiki_mask\n",
    "    combined_adj = torch.where(combined_mask < -1e8, torch.zeros_like(combined_adj), combined_adj)\n",
    "    \n",
    "    # If not using graph structure, return identity matrix\n",
    "    if not MODEL_CONFIG['use_graph_structure']:\n",
    "        return torch.eye(combined_adj.shape[0], device=device)\n",
    "    \n",
    "    # Normalize: row-wise normalization (each row sums to 1)\n",
    "    row_sums = combined_adj.sum(dim=1, keepdim=True)\n",
    "    adjacency_matrix = combined_adj / (row_sums + 1e-8)\n",
    "    \n",
    "    return adjacency_matrix.to(device)\n",
    "\n",
    "\n",
    "def build_separate_adjacency_matrices(industry_encodings, industry_mask, wiki_encodings, wiki_mask, device):\n",
    "    \"\"\"\n",
    "    Build SEPARATE normalized adjacency matrices for industry and wiki relations\n",
    "    Used when separate_edge_weights=True\n",
    "    \n",
    "    Returns:\n",
    "        industry_adj: [num_companies, num_companies]\n",
    "        wiki_adj: [num_companies, num_companies]\n",
    "    \"\"\"\n",
    "    # Build industry adjacency\n",
    "    industry_adj = torch.sum(industry_encodings, dim=-1) if MODEL_CONFIG['use_industry_relations'] else torch.zeros_like(industry_mask)\n",
    "    industry_adj = torch.where(industry_mask < -1e8, torch.zeros_like(industry_adj), industry_adj)\n",
    "    \n",
    "    # Build wiki adjacency\n",
    "    wiki_adj = torch.sum(wiki_encodings, dim=-1) if MODEL_CONFIG['use_wiki_relations'] else torch.zeros_like(wiki_mask)\n",
    "    wiki_adj = torch.where(wiki_mask < -1e8, torch.zeros_like(wiki_adj), wiki_adj)\n",
    "    \n",
    "    # If not using graph structure, return identity matrices\n",
    "    if not MODEL_CONFIG['use_graph_structure']:\n",
    "        identity = torch.eye(industry_adj.shape[0], device=device)\n",
    "        return identity, identity\n",
    "    \n",
    "    # Normalize each separately\n",
    "    industry_row_sums = industry_adj.sum(dim=1, keepdim=True)\n",
    "    industry_adj = industry_adj / (industry_row_sums + 1e-8)\n",
    "    \n",
    "    wiki_row_sums = wiki_adj.sum(dim=1, keepdim=True)\n",
    "    wiki_adj = wiki_adj / (wiki_row_sums + 1e-8)\n",
    "    \n",
    "    return industry_adj.to(device), wiki_adj.to(device)\n",
    "\n",
    "def prepare_data(eod_data, masks, base_price, device, window_size=20, prediction_horizon=1):\n",
    "    \"\"\"\n",
    "    Create sliding windows for time series prediction with mask handling\n",
    "    \n",
    "    Args:\n",
    "        eod_data: [num_companies, num_days, num_features]\n",
    "        masks: [num_companies, num_days] - 1.0 for valid, 0.0 for missing\n",
    "        base_price: [num_companies, num_days] - closing price of stock\n",
    "        window_size: Number of historical days to use as input\n",
    "        prediction_horizon: Number of days ahead to predict (usually 1)\n",
    "    \n",
    "    Returns:\n",
    "        X: Input windows [num_samples, num_companies, window_size, num_features]\n",
    "        y: Target prices [num_samples, num_companies, prediction_horizon]\n",
    "        sample_masks: Valid sample indicators [num_samples, num_companies]\n",
    "    \"\"\"\n",
    "    num_companies, num_days, num_features = eod_data.shape\n",
    "    num_samples = num_days - window_size - prediction_horizon + 1\n",
    "    \n",
    "    X = torch.zeros(num_samples, num_companies, window_size, num_features, device=device)\n",
    "    y = torch.zeros(num_samples, num_companies, prediction_horizon, device=device)\n",
    "    sample_masks = torch.zeros(num_samples, num_companies, device=device)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        X[i] = eod_data[:, i:i+window_size, :]\n",
    "        y[i, :, :] = base_price[:, i+window_size:i+window_size+prediction_horizon]\n",
    "        \n",
    "        # A sample is valid if all days in the window AND the target day are valid\n",
    "        window_valid = masks[:, i:i+window_size].min(dim=1)[0]  # [num_companies]\n",
    "        target_valid = masks[:, i+window_size:i+window_size+prediction_horizon].min(dim=1)[0]\n",
    "        sample_masks[i] = window_valid * target_valid\n",
    "    \n",
    "    return X, y, sample_masks\n",
    "\n",
    "\n",
    "def temporal_train_val_test_split(X, y, masks):\n",
    "    num_samples = X.shape[0]\n",
    "    \n",
    "    # Calculate split indices based on time\n",
    "    # train_end = int(num_samples * train_ratio)\n",
    "    # val_end = int(num_samples * (train_ratio + val_ratio))\n",
    "\n",
    "    train_end = train_size\n",
    "    val_end = train_size + validation_size\n",
    "    # Split data temporally\n",
    "    splits = {\n",
    "        'X_train': X[:train_size],\n",
    "        'y_train': y[:train_size],\n",
    "        'masks_train': masks[:train_size],\n",
    "        \n",
    "        'X_val': X[train_size:train_size+validation_size],\n",
    "        'y_val': y[train_size:train_size+validation_size],\n",
    "        'masks_val': masks[train_size:train_size+validation_size],\n",
    "        \n",
    "        'X_test': X[train_size+validation_size:],\n",
    "        'y_test': y[train_size+validation_size:],\n",
    "        'masks_test': masks[train_size+validation_size:],\n",
    "    }\n",
    "    \n",
    "    # Print split information\n",
    "    # TODO check this printing\n",
    "    print(f\"\\nTemporal Data Split:\")\n",
    "    print(f\"  Train: samples [0:{train_end}] = {splits['X_train'].shape[0]} timesteps\")\n",
    "    print(f\"  Val:   samples [{train_end}:{val_end}] = {splits['X_val'].shape[0]} timesteps\")\n",
    "    print(f\"  Test:  samples [{val_end}:{num_samples}] = {splits['X_test'].shape[0]} timesteps\")\n",
    "\n",
    "    print(f\"\\n  Valid train samples: {splits['masks_train'].sum().item():.0f} / {splits['masks_train'].numel()}\")\n",
    "    print(f\"  Valid val samples:   {splits['masks_val'].sum().item():.0f} / {splits['masks_val'].numel()}\")\n",
    "    print(f\"  Valid test samples:  {splits['masks_test'].sum().item():.0f} / {splits['masks_test'].numel()}\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "\n",
    "def adjacency_to_edges(adjacency_matrix, industry_adj=None, wiki_adj=None):\n",
    "    \"\"\"\n",
    "    Convert adjacency matrix to edge_index and edge_weight for PyTorch Geometric\n",
    "    Supports both 1D (combined) and 2D (separate industry/wiki) edge features\n",
    "    \n",
    "    Args:\n",
    "        adjacency_matrix: [num_companies, num_companies] tensor - combined adjacency\n",
    "        industry_adj: [num_companies, num_companies] tensor - industry adjacency (optional)\n",
    "        wiki_adj: [num_companies, num_companies] tensor - wiki adjacency (optional)\n",
    "        \n",
    "    Returns:\n",
    "        edge_index: [2, num_edges] - edge connectivity\n",
    "        edge_weight: [num_edges, 1] or [num_edges, 2] - edge weights\n",
    "    \"\"\"\n",
    "    adj_np = adjacency_matrix.cpu().numpy()\n",
    "    rows, cols = np.where(adj_np > 0)\n",
    "    \n",
    "    edge_index = torch.tensor(np.stack([rows, cols]), dtype=torch.long)\n",
    "    \n",
    "    # Check if we need 2D edge features\n",
    "    if MODEL_CONFIG['separate_edge_weights'] and industry_adj is not None and wiki_adj is not None:\n",
    "        # 2D edge features: [industry_weight, wiki_weight]\n",
    "        industry_np = industry_adj.cpu().numpy()\n",
    "        wiki_np = wiki_adj.cpu().numpy()\n",
    "        \n",
    "        industry_weights = industry_np[rows, cols]\n",
    "        wiki_weights = wiki_np[rows, cols]\n",
    "        \n",
    "        edge_weight = torch.tensor(\n",
    "            np.stack([industry_weights, wiki_weights], axis=1),  # [num_edges, 2]\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        print(f\"  Using 2D edge features: [industry, wiki]\")\n",
    "    else:\n",
    "        # 1D edge features: combined weight\n",
    "        edge_weights = adj_np[rows, cols]\n",
    "        edge_weight = torch.tensor(edge_weights, dtype=torch.float32).view(-1, 1)  # [num_edges, 1]\n",
    "        print(f\"  Using 1D edge features: combined\")\n",
    "    \n",
    "    return edge_index, edge_weight\n",
    "\n",
    "\n",
    "def create_batched_edges(edge_index, edge_weight, num_companies, max_batch_size):\n",
    "    \"\"\"\n",
    "    Creates one big graph with disconnected subgraphs for each timestep\n",
    "    \n",
    "    Args:\n",
    "        edge_index: [2, num_edges] - base edge connectivity\n",
    "        edge_weight: [num_edges, edge_dim] - base edge weights\n",
    "        num_companies: Number of nodes per timestep\n",
    "        max_batch_size: Maximum number of timesteps to support\n",
    "    \n",
    "    Returns:\n",
    "        edge_index_batched: [2, max_batch_size * num_edges]\n",
    "        edge_weight_batched: [max_batch_size * num_edges, edge_dim]\n",
    "    \"\"\"\n",
    "    num_edges = edge_index.shape[1]\n",
    "    edge_dim = edge_weight.shape[1]\n",
    "    \n",
    "    # Pre-allocate tensors\n",
    "    edge_index_batched = torch.zeros(2, max_batch_size * num_edges, dtype=torch.long, device=edge_index.device)\n",
    "    edge_weight_batched = torch.zeros(max_batch_size * num_edges, edge_dim, dtype=torch.float32, device=edge_weight.device)\n",
    "    \n",
    "    # Fill in edges for each timestep\n",
    "    for b in range(max_batch_size):\n",
    "        start_idx = b * num_edges\n",
    "        end_idx = (b + 1) * num_edges\n",
    "        \n",
    "        # Offset edge indices by b * num_companies\n",
    "        edge_index_batched[:, start_idx:end_idx] = edge_index + (b * num_companies)\n",
    "        edge_weight_batched[start_idx:end_idx] = edge_weight\n",
    "    \n",
    "    return edge_index_batched, edge_weight_batched\n",
    "\n",
    "\n",
    "def calculate_loss(predictions, targets, masks, criterion):\n",
    "    \"\"\"\n",
    "    Calculate masked loss (only compute loss on valid samples)\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions [batch, companies, output_dim]\n",
    "        targets: Ground truth [batch, companies, output_dim]\n",
    "        masks: Valid sample mask [batch, companies]\n",
    "        criterion: Loss function (should have reduction='none')\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss value\n",
    "    \"\"\"\n",
    "    loss_per_sample = criterion(predictions, targets)\n",
    "    masked_loss = loss_per_sample * masks.unsqueeze(-1)\n",
    "    num_valid = masks.sum() + 1e-8\n",
    "    loss = masked_loss.sum() / num_valid\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"Utility functions loaded successfully!\")"
   ],
   "metadata": {
    "id": "wgYbWmhHHA3u",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:42.854837Z",
     "iopub.execute_input": "2025-10-29T17:53:42.855015Z",
     "iopub.status.idle": "2025-10-29T17:53:42.866146Z",
     "shell.execute_reply.started": "2025-10-29T17:53:42.855002Z",
     "shell.execute_reply": "2025-10-29T17:53:42.865576Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True",
   "metadata": {
    "id": "H-6FODpFEYEy",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:42.876889Z",
     "iopub.execute_input": "2025-10-29T17:53:42.877144Z",
     "iopub.status.idle": "2025-10-29T17:53:43.011502Z",
     "shell.execute_reply.started": "2025-10-29T17:53:42.877120Z",
     "shell.execute_reply": "2025-10-29T17:53:43.010747Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# CONVERT DATA TO TENSORS\n# ============================================================================\n\nprint(f\"Converting data to tensors for {num_companies} companies...\")\n\n# Convert to tensors - use the already subsampled data\neod_data_tensor = torch.tensor(eod_data, dtype=torch.float32)\nmasks_tensor = torch.tensor(eod_masks, dtype=torch.float32)\nprice_prediction = torch.tensor(eod_base_price, dtype=torch.float32)\n\n# Relation data tensors\nindustry_encodings_tensor = torch.tensor(industry_encodings, dtype=torch.float32)\nindustry_mask_tensor = torch.tensor(industry_mask, dtype=torch.float32)\nwiki_encodings_tensor = torch.tensor(wiki_encodings, dtype=torch.float32)\nwiki_mask_tensor = torch.tensor(wiki_mask, dtype=torch.float32)\n\nprint(f\"\\nTensor shapes:\")\nprint(f\"  EOD data: {eod_data_tensor.shape}\")\nprint(f\"  Masks: {masks_tensor.shape}\")\nprint(f\"  Price prediction: {price_prediction.shape}\")\nprint(f\"  Industry encodings: {industry_encodings_tensor.shape}\")\nprint(f\"  Wiki encodings: {wiki_encodings_tensor.shape}\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Build graph structures\n",
    "adjacency_matrix = build_adjacency_matrix(\n",
    "    industry_encodings_tensor, industry_mask_tensor,\n",
    "    wiki_encodings_tensor, wiki_mask_tensor,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Build separate adjacency matrices if using 2D edge weights\n",
    "if BASE_MODEL_CONFIG['separate_edge_weights']:\n",
    "    industry_adj, wiki_adj = build_separate_adjacency_matrices(\n",
    "        industry_encodings_tensor, industry_mask_tensor,\n",
    "        wiki_encodings_tensor, wiki_mask_tensor,\n",
    "        device=device\n",
    "    )\n",
    "else:\n",
    "    industry_adj, wiki_adj = None, None\n",
    "\n",
    "# Prepare temporal data with sliding windows\n",
    "X_all, y_all, masks_all = prepare_data(\n",
    "    eod_data_tensor, masks_tensor, price_prediction,\n",
    "    window_size=window_size,\n",
    "    prediction_horizon=prediction_horizon,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nFull dataset prepared:\")\n",
    "print(f\"  X_all: {X_all.shape}\")\n",
    "print(f\"  y_all: {y_all.shape}\")\n",
    "print(f\"  masks_all: {masks_all.shape}\")\n",
    "\n",
    "# Split data temporally into train/val/test\n",
    "data_splits = temporal_train_val_test_split(\n",
    "    X_all, y_all, masks_all)\n",
    "\n",
    "# Extract splits for easier access\n",
    "X_train = data_splits['X_train']\n",
    "y_train = data_splits['y_train']\n",
    "masks_train = data_splits['masks_train']\n",
    "\n",
    "X_val = data_splits['X_val']\n",
    "y_val = data_splits['y_val']\n",
    "masks_val = data_splits['masks_val']\n",
    "\n",
    "X_test = data_splits['X_test']\n",
    "y_test = data_splits['y_test']\n",
    "masks_test = data_splits['masks_test']\n",
    "\n",
    "# Convert adjacency to edge format for GAT models\n",
    "print(f\"\\nConverting adjacency to edge format\")\n",
    "edge_index, edge_weight = adjacency_to_edges(adjacency_matrix, industry_adj, wiki_adj)\n",
    "edge_index = edge_index.to(device)\n",
    "edge_weight = edge_weight.to(device)\n",
    "\n",
    "# Pre-compute batched edges\n",
    "if BASE_MODEL_CONFIG['use_graph_concatenation']:\n",
    "    # Calculate max timesteps needed across all configurations\n",
    "    rnn_max_timesteps = the_max_batch_size * window_size  # is training samples * time steps, which results in the number of disconnected graphs used\n",
    "    non_rnn_max_timesteps = X_all.shape[0]  # All training samples since without rnn, we just flatten the timesteps * features. so we just have node features. And we have a graph per training sample.\n",
    "    \n",
    "    # Use the max to support multiple configs\n",
    "    max_timesteps = max(rnn_max_timesteps, non_rnn_max_timesteps)\n",
    "    \n",
    "    print(f\"  RNN mode would need: {rnn_max_timesteps} timesteps\")\n",
    "    print(f\"  Non-RNN mode would need: {non_rnn_max_timesteps} timesteps\")\n",
    "    print(f\"  Allocating for MAX: {max_timesteps} timesteps (supports both modes)\")\n",
    "\n",
    "    edge_index_batched, edge_weight_batched = create_batched_edges(\n",
    "        edge_index, edge_weight, num_companies, max_timesteps\n",
    "    )\n",
    "    print(f\"  Edge index batched: {edge_index_batched.shape}\")\n",
    "    print(f\"  Edge weight batched: {edge_weight_batched.shape}\")\n",
    "    print(f\"  Memory allocated: ~{(edge_index_batched.numel() * 8 + edge_weight_batched.numel() * 4) / 1024 / 1024:.1f} MB\")\n",
    "else:\n",
    "    edge_index_batched, edge_weight_batched = None, None\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Graph structure:\")\n",
    "print(f\"  Adjacency matrix: {adjacency_matrix.shape}\")\n",
    "print(f\"  Edge index: {edge_index.shape}\")\n",
    "print(f\"  Edge weight: {edge_weight.shape}\")\n",
    "print(f\"  Edge dimension: {edge_weight.shape[1]}D ({'industry+wiki separate' if edge_weight.shape[1] == 2 else 'combined'}) \")\n",
    "print(f\"  Total edges: {edge_index.shape[1]}\")\n",
    "print(f\"{'='*60}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feQ_WQ2JrkPY",
    "outputId": "1708b013-d82a-4ee3-f1c0-e501f50fef1a",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:54:48.397770Z",
     "iopub.execute_input": "2025-10-29T17:54:48.398489Z",
     "iopub.status.idle": "2025-10-29T17:54:48.999323Z",
     "shell.execute_reply.started": "2025-10-29T17:54:48.398465Z",
     "shell.execute_reply": "2025-10-29T17:54:48.998433Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Xzxly4ZFtin",
    "outputId": "cc2f23fb-21b6-43ad-c5c8-ae4d4a0d0e99",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:53:43.648698Z",
     "iopub.execute_input": "2025-10-29T17:53:43.649023Z",
     "iopub.status.idle": "2025-10-29T17:53:43.664586Z",
     "shell.execute_reply.started": "2025-10-29T17:53:43.649008Z",
     "shell.execute_reply": "2025-10-29T17:53:43.663741Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GATModel"
  },
  {
   "cell_type": "code",
   "source": [
    "class GATModel(nn.Module):\n",
    "    \"\"\"\n",
    "   Options:\n",
    "    1. Without RNN: GAT(flattened_time) → prediction\n",
    "    2. With RNN: GAT(t=1) → ... → GAT(t=20) → RNN → prediction\n",
    "    3. With RNN first, so RNN -> GAT(one embedding per window) -> prediction\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, layers_dim, num_companies, adjacency_matrix, device, edge_index, edge_weight,\n",
    "                 edge_index_batched=None, edge_weight_batched=None, window_size=20):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_companies = num_companies\n",
    "        self.layers_dim = layers_dim\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Graph structure\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "        self.mask = (self.adjacency_matrix == 0)\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_weight = edge_weight\n",
    "        \n",
    "        # Batched edges (for fast processing)\n",
    "        self.use_batched = MODEL_CONFIG['use_graph_concatenation'] and edge_index_batched is not None\n",
    "        if self.use_batched:\n",
    "            self.register_buffer('edge_index_batched', edge_index_batched)\n",
    "            self.register_buffer('edge_weight_batched', edge_weight_batched)\n",
    "        \n",
    "        # Determine edge dimension (1D or 2D)\n",
    "        edge_dim = edge_weight.shape[1]\n",
    "        \n",
    "        # Temporal modeling configuration\n",
    "        self.use_rnn = MODEL_CONFIG['use_rnn']\n",
    "        \n",
    "        if self.use_rnn:\n",
    "            # ===== WITH RNN: GAT outputs embeddings, RNN processes them temporally =====\n",
    "            gat_output_dim = MODEL_CONFIG['hidden_dim']\n",
    "            \n",
    "            self.conv_0 = GATConv(\n",
    "                in_channels=layers_dim[0][0],  # num_features (5)\n",
    "                out_channels=gat_output_dim,\n",
    "                heads=MODEL_CONFIG['num_attention_heads'],\n",
    "                concat=False,\n",
    "                dropout=MODEL_CONFIG['dropout'],\n",
    "                add_self_loops=True,\n",
    "                edge_dim=edge_dim,\n",
    "                fill_value=0,\n",
    "                bias=True\n",
    "            )\n",
    "            \n",
    "            # RNN processes temporal embeddings\n",
    "            rnn_input_dim = gat_output_dim\n",
    "            rnn_hidden_dim = MODEL_CONFIG['rnn_hidden_dim']\n",
    "            rnn_num_layers = MODEL_CONFIG['rnn_num_layers']\n",
    "\n",
    "            if MODEL_CONFIG['rnn_type'] == 'LSTM':\n",
    "                self.rnn = nn.LSTM(\n",
    "                    input_size=rnn_input_dim,\n",
    "                    hidden_size=rnn_hidden_dim,\n",
    "                    num_layers=rnn_num_layers,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=False,\n",
    "                    dropout=MODEL_CONFIG['dropout'] if rnn_num_layers > 1 else 0\n",
    "                )\n",
    "            else:  # GRU\n",
    "                self.rnn = nn.GRU(\n",
    "                    input_size=rnn_input_dim,\n",
    "                    hidden_size=rnn_hidden_dim,\n",
    "                    num_layers=rnn_num_layers,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=False,\n",
    "                    dropout=MODEL_CONFIG['dropout'] if rnn_num_layers > 1 else 0\n",
    "                )\n",
    "            \n",
    "            # Final projection from RNN output to prediction\n",
    "            rnn_output_dim = rnn_hidden_dim * 1\n",
    "            self.output_projection = nn.Linear(rnn_output_dim, layers_dim[0][1]) # output layer linear\n",
    "            \n",
    "        else:\n",
    "            # ===== WITHOUT RNN/TCN: GAT directly outputs prediction (original) =====\n",
    "            self.conv_0 = GATConv(\n",
    "                in_channels=layers_dim[0][0],  # num_features * window_size\n",
    "                out_channels=layers_dim[0][1], # prediction_horizon\n",
    "                heads=MODEL_CONFIG['num_attention_heads'],\n",
    "                concat=False,\n",
    "                dropout=MODEL_CONFIG['dropout'],\n",
    "                add_self_loops=True,\n",
    "                edge_dim=edge_dim,\n",
    "                fill_value=0,\n",
    "                bias=True\n",
    "            )\n",
    "        \n",
    "        # Optional ReLU activation\n",
    "        self.use_relu = MODEL_CONFIG['use_relu_output']\n",
    "        \n",
    "        # Store config for forward pass\n",
    "        self.rnn_type = MODEL_CONFIG.get('rnn_type', 'LSTM')\n",
    "\n",
    "        self.use_rnn_first = MODEL_CONFIG['use_rnn_first']\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Historical data [batch, num_companies, window_size, num_features]\n",
    "        Returns:\n",
    "            predictions: [batch, num_companies, output_dim]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0] # normally 4 now, number of training samples\n",
    "        \n",
    "        if self.use_rnn:\n",
    "            if self.use_rnn_first:\n",
    "                x_rnn_in = x.view(-1, self.window_size, x.shape[-1])  # [B*C, T, F]\n",
    "                # RNN forward pass\n",
    "                if self.rnn_type == 'LSTM':\n",
    "                    rnn_out, (h_n, c_n) = self.rnn(x_rnn_in)\n",
    "                else:  # GRU\n",
    "                    rnn_out, h_n = self.rnn(x_rnn_in)\n",
    "\n",
    "                # Use last hidden state\n",
    "                h_n = h_n[-1]\n",
    "                lstm_embeddings = h_n.view(batch_size, self.num_companies, -1)\n",
    "\n",
    "                if self.use_batched:\n",
    "                    x_flat = lstm_embeddings.view(-1, lstm_embeddings.shape[-1]) # does training samples * companies, so you get for every node the features from the lstm\n",
    "                    num_edges_per_graph = self.edge_index.shape[1] # is just the normal number of edges in the graph , edge index has shape [2, num_edges]\n",
    "                    total_edges = batch_size * num_edges_per_graph\n",
    "                    edge_idx = self.edge_index_batched[:, :total_edges]\n",
    "                    edge_wgt = self.edge_weight_batched[:total_edges]\n",
    "                    out = self.conv_0(x_flat, edge_idx, edge_wgt)\n",
    "                    out = out.view(batch_size, self.num_companies, -1)\n",
    "\n",
    "                else:\n",
    "                    outputs = []\n",
    "                    for b in range(batch_size):\n",
    "                        out_b = self.conv_0(lstm_embeddings[b], self.edge_index, self.edge_weight)\n",
    "                        outputs.append(out_b)\n",
    "                    out = torch.stack(outputs)\n",
    "            else:\n",
    "                # ===== WITH RNN: Compute GAT embeddings for each timestep =====\n",
    "                # (RNN implementation)\n",
    "                if self.use_batched:\n",
    "                    # FAST: Process all batch*window_size timesteps together\n",
    "                    x_reshaped = x.permute(0, 2, 1, 3).contiguous() # reorder to (training samples, time, companies, features) # todo maybe remove contiguous is maybe not needed\n",
    "                    x_flat = x_reshaped.view(-1, self.num_companies, x.shape[-1])  # create training_samples*timesteps, companies, features, so you have training_samples*timesteps number of graphs\n",
    "                    x_gat_input = x_flat.view(-1, x.shape[-1]) # now training_samples*timesteps*companies, features -> (training_samples*timesteps) number of subgraphs, now times the companies\n",
    "\n",
    "                    # since we now have training_samples*timesteps number of subgraphs, we need that many subgraph edge indices and weights\n",
    "                    # to select the right ones we do the following\n",
    "                    num_edges_per_timestep = self.edge_index.shape[1] # is just the normal number of edges in the graph , edge index has shape [2, num_edges]\n",
    "                    total_timesteps = batch_size * self.window_size # this is just training samples * timesteps, so we have total_timesteps or total number of subgraphs\n",
    "                    total_edges = total_timesteps * num_edges_per_timestep # just checking how many edges in total if we have total timesteps number of subgraphs\n",
    "\n",
    "                    edge_idx = self.edge_index_batched[:, :total_edges] # shape of edge_index_batched is [2, total_edges], now you keep the edges you need for all the subgraphs together, todo this is kinda not really needed since all graphs are the same but okay so can improve this still\n",
    "                    edge_wgt = self.edge_weight_batched[:total_edges] # here the same but just take the weights\n",
    "\n",
    "                    embeddings = self.conv_0(x_gat_input, edge_idx, edge_wgt) # run gatconv with (training samples * timesteps) disconnected graphs\n",
    "                    embeddings = embeddings.view(batch_size, self.window_size, self.num_companies, -1) # now turn back to training samples , timesteps, companies, features(features are here the output of gat)\n",
    "                    embeddings = embeddings.permute(0, 2, 1, 3).contiguous() # back to training samples, companies, timesteps, features\n",
    "                else:\n",
    "                    # SLOW: Process each timestep sequentially\n",
    "                    timestep_embeddings = []\n",
    "                    for t in range(self.window_size): # need to do per timestep because we need per timestep for the RNN\n",
    "                        x_t = x[:, :, t, :] # pick only current timestep for all training samples and companies, and pick all features\n",
    "                        batch_embeddings = []\n",
    "                        for b in range(batch_size): # for each training sample\n",
    "                            emb = self.conv_0(x_t[b], self.edge_index, self.edge_weight)\n",
    "                            batch_embeddings.append(emb)\n",
    "                        timestep_embeddings.append(torch.stack(batch_embeddings))\n",
    "                    embeddings = torch.stack(timestep_embeddings, dim=2)\n",
    "\n",
    "                # embeddings: [batch, companies, window_size, hidden_dim]\n",
    "                embeddings_rnn = embeddings.view(-1, self.window_size, embeddings.shape[-1]) # so will run per company\n",
    "\n",
    "                # RNN forward pass\n",
    "                if self.rnn_type == 'LSTM':\n",
    "                    rnn_out, (h_n, c_n) = self.rnn(embeddings_rnn)\n",
    "                else:  # GRU\n",
    "                    rnn_out, h_n = self.rnn(embeddings_rnn)\n",
    "\n",
    "                # Use last hidden state\n",
    "                h_n = h_n[-1]\n",
    "\n",
    "                # Project to output\n",
    "                out = self.output_projection(h_n)\n",
    "                out = out.view(batch_size, self.num_companies, -1)\n",
    "            \n",
    "        else:\n",
    "            # ===== WITHOUT RNN: Original architecture (flatten time) =====\n",
    "            x_reshaped = x.view(batch_size, self.num_companies, -1)\n",
    "            \n",
    "            if self.use_batched:\n",
    "                # FAST BATCHED PROCESSING\n",
    "                x_flat = x_reshaped.view(-1, x_reshaped.shape[-1]) # gives (training samples * companies) which all have features\n",
    "                \n",
    "                num_edges_per_timestep = self.edge_index.shape[1] # is just the normal number of edges in the graph , edge index has shape [2, num_edges]\n",
    "                total_edges = batch_size * num_edges_per_timestep # total edges when accoutning for the batch size\n",
    "                \n",
    "                edge_idx = self.edge_index_batched[:, :total_edges]\n",
    "                edge_wgt = self.edge_weight_batched[:total_edges]\n",
    "                \n",
    "                out = self.conv_0(x_flat, edge_idx, edge_wgt)\n",
    "                out = out.view(batch_size, self.num_companies, -1) # convert back to batch, companies, features\n",
    "            else:\n",
    "                # SLOW SEQUENTIAL PROCESSING\n",
    "                outputs = []\n",
    "                for b in range(batch_size): # every training sample separate\n",
    "                    out_b = self.conv_0(x_reshaped[b], self.edge_index, self.edge_weight) # output of gat layer\n",
    "                    outputs.append(out_b)\n",
    "                out = torch.stack(outputs)\n",
    "        \n",
    "        # Optional ReLU activation (off by default for regression)\n",
    "        if self.use_relu:\n",
    "            out = F.relu(out)\n",
    "        \n",
    "        return out\n"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Grid search"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Storage for results\n",
    "grid_search_results = []\n",
    "all_epoch_losses = {}  # Store per-epoch losses for plotting\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"start grid search - {len(CONFIGS_TO_TEST)} configs\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for config_idx, config_updates in enumerate(CONFIGS_TO_TEST, 1):\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"# configs {config_idx}/{len(CONFIGS_TO_TEST)}: {config_updates['name']}\")\n",
    "    print(f\"{'#'*80}\\n\")\n",
    "\n",
    "    # Update MODEL_CONFIG with this configuration\n",
    "    MODEL_CONFIG = BASE_MODEL_CONFIG.copy()\n",
    "    MODEL_CONFIG.update(config_updates)\n",
    "\n",
    "    # Print configuration\n",
    "    print(f\"Configuration details:\")\n",
    "    for key, value in config_updates.items():\n",
    "        if key != 'name':\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Verify features\n",
    "    print(f\"\\nFeature verification:\")\n",
    "    print(f\"  Input shape: {X_train.shape}\")\n",
    "    print(f\"  Features per timestep: {X_train.shape[3]} (should be {num_features})\")\n",
    "    assert X_train.shape[3] == num_features, f\"Expected {num_features} features but got {X_train.shape[3]}\"\n",
    "    print(f\"  All {num_features} features are being used!\")\n",
    "\n",
    "    # Initialize model\n",
    "    print(f\"\\nInitializing model...\")\n",
    "    model = GATModel(\n",
    "        layers_dim=[(num_features * window_size, prediction_horizon)] if (not MODEL_CONFIG['use_rnn'])\n",
    "                   else [(num_features, MODEL_CONFIG['rnn_hidden_dim'])],\n",
    "        num_companies=num_companies,\n",
    "        adjacency_matrix=adjacency_matrix,\n",
    "        device=device,\n",
    "        edge_index=edge_index,\n",
    "        edge_weight=edge_weight,\n",
    "        edge_index_batched=edge_index_batched if MODEL_CONFIG['use_graph_concatenation'] else None,\n",
    "        edge_weight_batched=edge_weight_batched if MODEL_CONFIG['use_graph_concatenation'] else None,\n",
    "        window_size=window_size,\n",
    "    ).to(device)\n",
    "\n",
    "    # Training configuration\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=MODEL_CONFIG['learning_rate'],\n",
    "        weight_decay=MODEL_CONFIG['weight_decay']\n",
    "    )\n",
    "\n",
    "    print(f\"Model details:\")\n",
    "    if MODEL_CONFIG['use_rnn']:\n",
    "        if MODEL_CONFIG['use_rnn_first']:\n",
    "            print(f\"  Architecture: RNN → GAT → Prediction\")\n",
    "        else:\n",
    "            print(f\"  Architecture: GAT → RNN → Prediction\")\n",
    "        print(f\"  RNN type: {MODEL_CONFIG['rnn_type']}\")\n",
    "        print(f\"  RNN hidden dim: {MODEL_CONFIG['rnn_hidden_dim']}\")\n",
    "    else:\n",
    "        print(f\"  Architecture: GAT → Prediction (flattened time)\")\n",
    "    print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  Learning rate: {MODEL_CONFIG['learning_rate']}\")\n",
    "\n",
    "    # Training tracking\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0 #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "    batch_size = MODEL_CONFIG['batch_size']\n",
    "\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    print(f\"{'-'*60}\\n\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        num_training_samples = X_train.shape[0]\n",
    "        total_train_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for i in range(0, num_training_samples, batch_size): # do it in batches\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = X_train[i:i+batch_size]\n",
    "            batch_y = y_train[i:i+batch_size]\n",
    "            batch_masks = masks_train[i:i+batch_size]\n",
    "            predictions = model(batch_X)\n",
    "\n",
    "            train_loss = calculate_loss(predictions, batch_y, batch_masks, criterion)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += train_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_train_loss = total_train_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # ===== VALIDATION =====\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        num_val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, X_val.shape[0], batch_size):\n",
    "                batch_X = X_val[i:i+batch_size]\n",
    "                batch_y = y_val[i:i+batch_size]\n",
    "                batch_masks = masks_val[i:i+batch_size]\n",
    "                val_predictions = model(batch_X)\n",
    "                val_loss = calculate_loss(val_predictions, batch_y, batch_masks, criterion)\n",
    "                total_val_loss += val_loss.item()\n",
    "                num_val_batches += 1\n",
    "\n",
    "        avg_val_loss = total_val_loss / num_val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # test\n",
    "        total_test_loss = 0\n",
    "        num_test_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, X_test.shape[0], batch_size):\n",
    "                batch_X = X_test[i:i+batch_size]\n",
    "                batch_y = y_test[i:i+batch_size]\n",
    "                batch_masks = masks_test[i:i+batch_size]\n",
    "                test_predictions = model(batch_X)\n",
    "                test_loss = calculate_loss(test_predictions, batch_y, batch_masks, criterion)\n",
    "                total_test_loss += test_loss.item()\n",
    "                num_test_batches += 1\n",
    "\n",
    "        avg_test_loss = total_test_loss / num_test_batches\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1:3d}/{epochs}] | Train: {avg_train_loss:.6f} | Val: {avg_val_loss:.6f} | Test: {avg_test_loss:.6f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"FINAL EVALUATION\")\n",
    "    print(f\"{'-'*60}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Train set\n",
    "        all_train_preds = []\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            batch_X = X_train[i:i+batch_size]\n",
    "            all_train_preds.append(model(batch_X))\n",
    "        train_predictions = torch.cat(all_train_preds, dim=0)\n",
    "        final_train_loss = calculate_loss(train_predictions, y_train, masks_train, criterion)\n",
    "\n",
    "        # Validation set\n",
    "        all_val_preds = []\n",
    "        for i in range(0, X_val.shape[0], batch_size):\n",
    "            batch_X = X_val[i:i+batch_size]\n",
    "            all_val_preds.append(model(batch_X))\n",
    "        val_predictions = torch.cat(all_val_preds, dim=0)\n",
    "        final_val_loss = calculate_loss(val_predictions, y_val, masks_val, criterion)\n",
    "\n",
    "        # Test set\n",
    "        all_test_preds = []\n",
    "        for i in range(0, X_test.shape[0], batch_size):\n",
    "            batch_X = X_test[i:i+batch_size]\n",
    "            all_test_preds.append(model(batch_X))\n",
    "        test_predictions = torch.cat(all_test_preds, dim=0)\n",
    "        final_test_loss = calculate_loss(test_predictions, y_test, masks_test, criterion)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    train_rmse = torch.sqrt(final_train_loss).item()\n",
    "    val_rmse = torch.sqrt(final_val_loss).item()\n",
    "    test_rmse = torch.sqrt(final_test_loss).item()\n",
    "\n",
    "    print(f\"\\nFinal RMSE:\")\n",
    "    print(f\"  Train: {train_rmse:.6f}\")\n",
    "    print(f\"  Val:   {val_rmse:.6f}\")\n",
    "    print(f\"  Test:  {test_rmse:.6f}\")\n",
    "\n",
    "    # Store per-epoch losses for this model\n",
    "    model_name = config_updates['name']\n",
    "    all_epoch_losses[model_name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'test_losses': test_losses,\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    result = {\n",
    "        'name': config_updates['name'],\n",
    "        'train_rmse': train_rmse,\n",
    "        'val_rmse': val_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mse': final_train_loss.item(),\n",
    "        'val_mse': final_val_loss.item(),\n",
    "        'test_mse': final_test_loss.item(),\n",
    "        'epochs_trained': len(train_losses),\n",
    "        'config': config_updates.copy(),\n",
    "        'train_pred': train_predictions.cpu().numpy(),\n",
    "        'val_pred': val_predictions.cpu().numpy(),\n",
    "        'test_pred': test_predictions.cpu().numpy(),\n",
    "    }\n",
    "    grid_search_results.append(result)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    model_path = f\"models/{config_updates['name']}.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"\\nModel saved to: {model_path}\")\n",
    "\n",
    "    # Clean up\n",
    "    del model, optimizer, criterion\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\n{'#'*80}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# grid ssearch summary\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"GRID SEARCH COMPLETE - RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(grid_search_results)\n",
    "results_df = results_df.sort_values('val_rmse')\n",
    "\n",
    "print(\"Results sorted by validation RMSE (best first):\\n\")\n",
    "print(results_df[['name', 'train_rmse', 'val_rmse', 'test_rmse', 'epochs_trained']].to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(f\"BEST MODEL: {results_df.iloc[0]['name']}\")\n",
    "print(f\"  Train RMSE: {results_df.iloc[0]['train_rmse']:.6f}\")\n",
    "print(f\"  Val RMSE:   {results_df.iloc[0]['val_rmse']:.6f}\")\n",
    "print(f\"  Test RMSE:  {results_df.iloc[0]['test_rmse']:.6f}\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('grid_search_results.csv', index=False)\n",
    "print(f\"\\nResults saved to: grid_search_results.csv\")\n",
    "print(f\"{'='*80}\\n\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T17:58:40.470365Z",
     "iopub.execute_input": "2025-10-29T17:58:40.470885Z",
     "iopub.status.idle": "2025-10-29T17:58:40.482898Z",
     "shell.execute_reply.started": "2025-10-29T17:58:40.470862Z",
     "shell.execute_reply": "2025-10-29T17:58:40.482205Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot + eval"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baseline model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For baseline: prediction = last price in the window\n",
    "# X_train shape: [num_samples, num_companies, window_size, num_features]\n",
    "# y_train shape: [num_samples, num_companies, prediction_horizon]\n",
    "\n",
    "def evaluate_baseline(X, y, masks):\n",
    "    \"\"\"Baseline: predict tomorrow's price = today's closing price\"\"\"\n",
    "    # Last price in window is X[:, :, -1, -1] (last timestep, last feature = close price)\n",
    "    baseline_predictions = X[:, :, -1, -1].unsqueeze(-1)  # [batch, companies, 1]\n",
    "\n",
    "    # Calculate loss\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    loss = calculate_loss(baseline_predictions, y, masks, criterion)\n",
    "    rmse = torch.sqrt(loss).item()\n",
    "\n",
    "    return loss.item(), rmse, baseline_predictions\n",
    "\n",
    "# Evaluate baseline on all splits\n",
    "baseline_train_mse, baseline_train_rmse, baseline_train_pred = evaluate_baseline(X_train, y_train, masks_train)\n",
    "baseline_val_mse, baseline_val_rmse, baseline_val_pred = evaluate_baseline(X_val, y_val, masks_val)\n",
    "baseline_test_mse, baseline_test_rmse, baseline_test_pred = evaluate_baseline(X_test, y_test, masks_test)\n",
    "\n",
    "print(f\"\\nBaseline Model Results:\")\n",
    "print(f\"  Train RMSE: {baseline_train_rmse:.6f}\")\n",
    "print(f\"  Val RMSE:   {baseline_val_rmse:.6f}\")\n",
    "print(f\"  Test RMSE:  {baseline_test_rmse:.6f}\")\n",
    "\n",
    "# Store baseline results for plotting\n",
    "baseline_result = {\n",
    "    'name': 'Baseline (tomorrow=today)',\n",
    "    'train_rmse': baseline_train_rmse,\n",
    "    'val_rmse': baseline_val_rmse,\n",
    "    'test_rmse': baseline_test_rmse,\n",
    "    'train_mse': baseline_train_mse,\n",
    "    'val_mse': baseline_val_mse,\n",
    "    'test_mse': baseline_test_mse,\n",
    "    'epochs_trained': 0,\n",
    "    'train_pred': baseline_train_pred.cpu().numpy(),\n",
    "    'val_pred': baseline_val_pred.cpu().numpy(),\n",
    "    'test_pred': baseline_test_pred.cpu().numpy(),\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create graph with on the x axis the number of epochs and the loss on the y axi"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if 'all_epoch_losses' in locals() and len(all_epoch_losses) > 0:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Plot 1: Training Loss\n",
    "    ax1 = axes[0]\n",
    "    for model_name, losses in all_epoch_losses.items():\n",
    "        epochs_range = range(1, len(losses['train_losses']) + 1)\n",
    "        ax1.plot(epochs_range, losses['train_losses'], label=model_name, alpha=0.7, linewidth=2)\n",
    "\n",
    "    # Add baseline (horizontal line since it doesn't train)\n",
    "    ax1.axhline(y=baseline_train_mse, color='black', linestyle='--',\n",
    "                linewidth=2, label='Baseline (tomorrow=today)', alpha=0.8)\n",
    "\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('MSE Loss', fontsize=12)\n",
    "    ax1.set_title(f'Training Loss vs Epoch - {market}', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='best', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Validation Loss\n",
    "    ax2 = axes[1]\n",
    "    for model_name, losses in all_epoch_losses.items():\n",
    "        epochs_range = range(1, len(losses['val_losses']) + 1)\n",
    "        ax2.plot(epochs_range, losses['val_losses'], label=model_name, alpha=0.7, linewidth=2)\n",
    "\n",
    "    # Add baseline\n",
    "    ax2.axhline(y=baseline_val_mse, color='black', linestyle='--',\n",
    "                linewidth=2, label='Baseline (tomorrow=today)', alpha=0.8)\n",
    "\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('MSE Loss', fontsize=12)\n",
    "    ax2.set_title(f'Validation Loss vs Epoch - {market}', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='best', fontsize=9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Test Loss\n",
    "    ax3 = axes[2]\n",
    "    for model_name, losses in all_epoch_losses.items():\n",
    "        epochs_range = range(1, len(losses['test_losses']) + 1)\n",
    "        ax3.plot(epochs_range, losses['test_losses'], label=model_name, alpha=0.7, linewidth=2)\n",
    "\n",
    "    # Add baseline\n",
    "    ax3.axhline(y=baseline_test_mse, color='black', linestyle='--',\n",
    "                linewidth=2, label='Baseline (tomorrow=today)', alpha=0.8)\n",
    "\n",
    "    ax3.set_xlabel('Epoch', fontsize=12)\n",
    "    ax3.set_ylabel('MSE Loss', fontsize=12)\n",
    "    ax3.set_title(f'Test Loss vs Epoch - {market}', fontsize=14, fontweight='bold')\n",
    "    ax3.legend(loc='best', fontsize=9)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{market}_epoch_vs_loss_all_models.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nVisualization saved to: {market}_epoch_vs_loss_all_models.png\")\n",
    "else:\n",
    "    print(\"No epoch losses available. Please run the grid search first.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot predition function"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_predictions(pred_price, actual_price,\n",
    "                     start_idx, num_companies_to_plot=5):\n",
    "    \"\"\"\n",
    "    Plot predicted vs actual stock prices for random companies\n",
    "\n",
    "    Args:\n",
    "        pred_price: Predicted prices [num_samples, num_companies, prediction_horizon]\n",
    "        actual_price: Actual prices [num_samples, num_companies, prediction_horizon]\n",
    "        start_idx: Starting day index in the original timeline\n",
    "        num_companies_to_plot: Number of companies to visualize\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    num_samples, num_companies, prediction_horizon = pred_price.shape\n",
    "\n",
    "    # Select 5 random companies\n",
    "    np.random.seed(40)\n",
    "    selected_companies = np.random.choice(num_companies, num_companies_to_plot, replace=False)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_companies_to_plot, 1, figsize=(12, 3*num_companies_to_plot))\n",
    "    if num_companies_to_plot == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, company_idx in enumerate(selected_companies):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        actual_prices = actual_price[:, company_idx, 0]\n",
    "        pred_prices = pred_price[:, company_idx, 0]\n",
    "        time_steps = np.arange(start_idx, start_idx + len(actual_prices))\n",
    "\n",
    "        # Plot\n",
    "        ax.plot(time_steps, actual_prices, label='Actual Price',\n",
    "                color='blue', linewidth=2, alpha=0.7)\n",
    "        ax.plot(time_steps, pred_prices, label='Predicted Price',\n",
    "                color='red', linewidth=2, linestyle='--', alpha=0.7)\n",
    "\n",
    "        ax.set_xlabel('Day Index')\n",
    "        ax.set_ylabel('Price')\n",
    "        ax.set_title(f'Company {company_idx}: Predicted vs Actual Stock Price')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('stock_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"Plot saved as 'stock_predictions.png'\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Plot predictions function loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# Plot predictions for best model\n",
    "# ============================================================================\n",
    "\n",
    "if 'grid_search_results' in locals() and len(grid_search_results) > 0:\n",
    "    # Get best model (lowest validation RMSE)\n",
    "    best_model = min(grid_search_results, key=lambda x: x['val_rmse'])\n",
    "    \n",
    "    print(f\"Plotting predictions for best model: {best_model['name']}\")\n",
    "    print(f\"Test RMSE: {best_model['test_rmse']:.6f}\\n\")\n",
    "    \n",
    "    # Get predictions and actual prices for test set\n",
    "    pred_test = best_model['test_pred']  # [num_samples, num_companies, prediction_horizon]\n",
    "    actual_test = y_test.cpu().numpy()\n",
    "    \n",
    "    # Calculate starting index for test set (after train and val)\n",
    "    train_end = train_size\n",
    "    val_end = train_size + validation_size\n",
    "    test_start_idx = val_end + window_size  # todo check this ???\n",
    "    \n",
    "    # Plot predictions\n",
    "    plot_predictions(pred_test, actual_test, \n",
    "                     start_idx=test_start_idx, \n",
    "                     num_companies_to_plot=5)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Saving all"
  },
  {
   "cell_type": "code",
   "source": [
    "#Saving all\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Prepare actual prices (ground truth)\n",
    "actual_prices = {\n",
    "    'train': y_train.cpu().numpy(),  # [num_samples, num_companies, prediction_horizon]\n",
    "    'val': y_val.cpu().numpy(),\n",
    "    'test': y_test.cpu().numpy(),\n",
    "}\n",
    "\n",
    "# Prepare all model predictions (including baseline)\n",
    "all_predictions = {}\n",
    "\n",
    "# Add baseline\n",
    "all_predictions['Baseline (tomorrow=today)'] = {\n",
    "    'train': baseline_result['train_pred'],\n",
    "    'val': baseline_result['val_pred'],\n",
    "    'test': baseline_result['test_pred'],\n",
    "}\n",
    "\n",
    "# Add all grid search models\n",
    "for result in grid_search_results:\n",
    "    all_predictions[result['name']] = {\n",
    "        'train': result['train_pred'],\n",
    "        'val': result['val_pred'],\n",
    "        'test': result['test_pred'],\n",
    "    }\n",
    "\n",
    "# Save to file for portfolio evaluation\n",
    "save_data = {\n",
    "    'actual_prices': actual_prices,\n",
    "    'predictions': all_predictions,\n",
    "    'market': market,\n",
    "    'num_companies': num_companies,\n",
    "    'window_size': window_size,\n",
    "    'train_size': train_size,\n",
    "    'validation_size': validation_size,\n",
    "    'test_size': test_size,\n",
    "}\n",
    "\n",
    "# Save as pickle for easy loading\n",
    "with open(f'{market}_predictions_and_actuals.pkl', 'wb') as f:\n",
    "    pickle.dump(save_data, f)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SAVED PREDICTIONS FOR PORTFOLIO EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"File: {market}_predictions_and_actuals.pkl\")\n",
    "print(f\"\\nContents:\")\n",
    "print(f\"  - Actual prices (train/val/test)\")\n",
    "print(f\"  - Predictions for {len(all_predictions)} models\")\n",
    "print(f\"  - Metadata (market, num_companies, splits, etc.)\")\n",
    "print(f\"\\nTo load in portfolio code:\")\n",
    "print(f\"  import pickle\")\n",
    "print(f\"  with open('{market}_predictions_and_actuals.pkl', 'rb') as f:\")\n",
    "print(f\"      data = pickle.load(f)\")\n",
    "print(f\"  actual_test = data['actual_prices']['test']\")\n",
    "print(f\"  pred_test = data['predictions']['GAT_LSTM']['test']\")\n",
    "print(f\"{'='*60}\\n\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
